{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.716651Z",
     "start_time": "2020-04-30T14:39:24.372938Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import empty, zeros\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.727560Z",
     "start_time": "2020-04-30T14:39:24.717586Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7faa7927ddf0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.733543Z",
     "start_time": "2020-04-30T14:39:24.729555Z"
    }
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def update(self, lr):\n",
    "        pass\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass\n",
    "    \n",
    "    def init_params(self, xavier_init, xavier_gain):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.745550Z",
     "start_time": "2020-04-30T14:39:24.734541Z"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, N_in, N_out, xavier_init=False, xavier_gain=1):\n",
    "        Module.__init__(self)\n",
    "        self.N_in = N_in\n",
    "        self.N_out = N_out\n",
    "        \n",
    "        self.init_params(xavier_init, xavier_gain)\n",
    "        \n",
    "        self.gradW = zeros(self.W.shape)\n",
    "        self.gradb = zeros(self.b.shape)\n",
    "        \n",
    "        self.m_weights = zeros(self.gradW.shape)\n",
    "        self.m_bias = zeros(self.gradb.shape)\n",
    "        \n",
    "        self.v_weights = zeros(self.gradW.shape)\n",
    "        self.v_bias = zeros(self.gradb.shape)\n",
    "    \n",
    "    def init_params(self, xavier_init=True, xavier_gain=1):\n",
    "        if xavier_init:\n",
    "            xavier_std = xavier_gain * math.sqrt(2.0 / (self.N_in + self.N_out))\n",
    "        else:\n",
    "            xavier_std = 1\n",
    "            \n",
    "        self.W = empty((self.N_in, self.N_out)).normal_(0, xavier_std)\n",
    "        self.b = empty((1, self.N_out)).normal_(0, xavier_std)\n",
    "    \n",
    "    def forward(self, *input_):\n",
    "        # out = W * input + b\n",
    "        x = input_[0].clone()\n",
    "        \n",
    "        self.x = x\n",
    "        \n",
    "        return self.x.mm(self.W) + self.b\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # grad_w += input * x^(l-1).t()\n",
    "        # grad_b += input\n",
    "        # out = w.t() * input\n",
    "        # input = grad of activation function, i.e. dl/ds^(l)\n",
    "        # x^(l-1) = input of the forward pass\n",
    "        input_ = gradwrtoutput[0].clone()\n",
    "        \n",
    "        self.gradW += self.x.t().mm(input_)\n",
    "        self.gradb += input_.sum(0)\n",
    "        \n",
    "        return input_.mm(self.W.t())\n",
    "        \n",
    "    def param(self):\n",
    "        return [(self.W, self.gradW, self.m_weights, self.v_weights), (self.b, self.gradb, self.m_bias, self.v_bias)]\n",
    "    \n",
    "    def update(self, lr):\n",
    "        self.W.sub_(lr * self.gradW)\n",
    "        self.b.sub_(lr * self.gradb)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.gradW = zeros(self.W.shape)\n",
    "        self.gradb = zeros(self.b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.755532Z",
     "start_time": "2020-04-30T14:39:24.746509Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self, ):\n",
    "        Module.__init__(self)\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        s = input_[0].clone()\n",
    "        self.s = s\n",
    "        \n",
    "        s[s < 0] = 0.\n",
    "        \n",
    "        return s\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # out = f'(s^(l)) * input\n",
    "        # s^(l) = input of forward pass\n",
    "        # input = grad of next layer\n",
    "        input_ = gradwrtoutput[0].clone()\n",
    "        \n",
    "        out = self.s.clone()\n",
    "        out[out > 0] = 1\n",
    "        out[out < 0] = 0\n",
    "        \n",
    "        \n",
    "        return out.mul(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.765459Z",
     "start_time": "2020-04-30T14:39:24.756482Z"
    }
   },
   "outputs": [],
   "source": [
    "class LeakyReLU(Module):\n",
    "    def __init__(self, alpha=0.01):\n",
    "        Module.__init__(self)\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        s = input_[0].clone()\n",
    "        self.s = s\n",
    "        \n",
    "        s[s < 0] = self.alpha * s[s < 0]\n",
    "        \n",
    "        return s\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # out = f'(s^(l)) * input\n",
    "        # s^(l) = input of forward pass\n",
    "        # input = grad of next layer\n",
    "        input_ = gradwrtoutput[0].clone()\n",
    "        \n",
    "        out = self.s.clone()\n",
    "        out[out > 0] = 1\n",
    "        out[out < 0] = self.alpha\n",
    "        \n",
    "        \n",
    "        return out.mul(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.774467Z",
     "start_time": "2020-04-30T14:39:24.767452Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        s = input_[0].clone()\n",
    "        self.s = s\n",
    "        \n",
    "        return s.tanh()\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # out = f'(s^(l)) * input\n",
    "        # s^(l) = input of forward pass\n",
    "        # input = grad of next layer\n",
    "        input_ = gradwrtoutput[0].clone()\n",
    "        \n",
    "        out = self.s.clone()\n",
    "        out = 1 - out.tanh().pow(2)\n",
    "        \n",
    "        return out.mul(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.785440Z",
     "start_time": "2020-04-30T14:39:24.775432Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        s = input_[0].clone()\n",
    "        self.s = s\n",
    "        \n",
    "        return s.sigmoid()\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # out = f'(s^(l)) * input\n",
    "        # s^(l) = input of forward pass\n",
    "        # input = grad of next layer\n",
    "        input_ = gradwrtoutput[0].clone()\n",
    "        \n",
    "        out = self.s.clone()\n",
    "        out = out.sigmoid() * (1 - out.sigmoid())\n",
    "        \n",
    "        return out.mul(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.794418Z",
     "start_time": "2020-04-30T14:39:24.786401Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self, *modules, xavier_init=None, xavier_gain=1):\n",
    "        Module.__init__(self)\n",
    "        self.modules = modules\n",
    "        self.xavier_init = xavier_init\n",
    "        if xavier_init is not None:\n",
    "            for module in self.modules:\n",
    "                module.init_params(xavier_init, xavier_gain)\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        x = input_[0].clone()\n",
    "        \n",
    "        for m in self.modules:\n",
    "            x = m.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        x = gradwrtoutput[0].clone()\n",
    "        \n",
    "        for i, m in enumerate(reversed(self.modules)):\n",
    "            #print(\"{} : {}\".format(i, x))\n",
    "            x = m.backward(x)\n",
    "        \n",
    "    def param(self):\n",
    "        params = []\n",
    "        \n",
    "        for m in self.modules:\n",
    "            for param in m.param():\n",
    "                params.append(param)\n",
    "        \n",
    "        return params\n",
    "\n",
    "    def update(self, lr):\n",
    "        for m in self.modules:\n",
    "            m.update(lr)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for m in self.modules:\n",
    "            m.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.804520Z",
     "start_time": "2020-04-30T14:39:24.795378Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"class Loss: \n",
    "    ...\n",
    "\"\"\"\n",
    "class LossMSE(Module):\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        \n",
    "    def forward(self, y, target):\n",
    "        # out = e^2\n",
    "        # e = (y - f(x))\n",
    "        self.y = y.clone()\n",
    "        target_onehot = zeros((target.shape[0], 2)) \n",
    "        self.target = target_onehot.scatter_(1, target.view(-1, 1), 1)\n",
    "        \n",
    "        self.e = (self.y - self.target)\n",
    "        self.n = self.y.size(0)\n",
    "        \n",
    "        \n",
    "        return self.e.pow(2).sum()\n",
    "        \n",
    "    def backward(self):\n",
    "        # out = 2 * e\n",
    "        \n",
    "        return 2 * self.e # / self.n        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.815176Z",
     "start_time": "2020-04-30T14:39:24.805199Z"
    }
   },
   "outputs": [],
   "source": [
    "class LossCrossEntropy(Module):\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        \n",
    "    def forward(self, y, target):\n",
    "        self.y = y.clone()\n",
    "        self.target = target.clone()\n",
    "        sm = torch.softmax(self.y, dim=1)\n",
    "        likelihood = -torch.log(torch.clamp(sm[range(target.size(0)), target], min=1e-3, max=None))\n",
    "        return likelihood.mean()\n",
    "        \n",
    "    def backward(self):\n",
    "        sm = torch.softmax(self.y, dim=1)\n",
    "        target_onehot = zeros((self.target.shape[0], 2)) \n",
    "        target_onehot = target_onehot.scatter_(1, self.target.view(-1, 1), 1)\n",
    "        return sm - target_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.826287Z",
     "start_time": "2020-04-30T14:39:24.816173Z"
    }
   },
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, model, nb_epochs, mini_batch_size, lr, criterion):\n",
    "        self.model = model\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.lr = lr\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.criterion = criterion\n",
    "    \n",
    "    def train(self, train_input, train_target, verbose=True):\n",
    "        for e in range(self.nb_epochs):\n",
    "            sum_loss = 0.\n",
    "        \n",
    "            for b in range(0, train_input.size(0), self.mini_batch_size):\n",
    "                self.model.zero_grad()\n",
    "                \n",
    "                output = self.model.forward(train_input.narrow(0, b, self.mini_batch_size))\n",
    "                loss = self.criterion.forward(output, train_target.narrow(0, b, self.mini_batch_size))\n",
    "            \n",
    "                sum_loss += loss\n",
    "            \n",
    "                l_grad = self.criterion.backward()\n",
    "                self.model.backward(l_grad)\n",
    "                self.step()\n",
    "                \n",
    "            if verbose:\n",
    "                print(\"{} iteration: loss={}\".format(e, sum_loss))\n",
    "        return self.model\n",
    "    \n",
    "    def step(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, model, nb_epochs = 50, mini_batch_size=1, lr=1e-4, criterion=LossMSE()):\n",
    "        Optimizer.__init__(self, model, nb_epochs, mini_batch_size, lr, criterion)\n",
    "    \n",
    "    def step(self):\n",
    "        self.model.update(self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, model, nb_epochs = 50, mini_batch_size=1, lr=1e-3, criterion=LossMSE(), \n",
    "                 b1=0.9, b2=0.999, epsilon=1e-8):\n",
    "        Optimizer.__init__(self, model, nb_epochs, mini_batch_size, lr, criterion)\n",
    "        \n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        \n",
    "        for (param, grad, m, v) in self.model.param():\n",
    "            g = grad.clone()\n",
    "            \n",
    "            m = self.b1 * m + (1 - self.b1) * g\n",
    "            v = self.b2 * v + (1 - self.b2) * g.pow(2)\n",
    "            \n",
    "            m_biasc = m / (1 - self.b1 ** self.t)\n",
    "            v_biasc = v / (1 - self.b2 ** self.t)\n",
    "            \n",
    "            param.sub_(self.lr * m_biasc/(v_biasc.sqrt() + self.epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.836156Z",
     "start_time": "2020-04-30T14:39:24.827142Z"
    }
   },
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def cross_validate(self, k, values):\n",
    "        pass\n",
    "    \n",
    "    def test(self, test_input, test_target):\n",
    "        num_samples = test_input.size(0)\n",
    "        prediction = self.model.forward(test_input)\n",
    "        predicted_class = torch.argmax(prediction, axis=1)\n",
    "        accuracy = sum(predicted_class == test_target).float() / num_samples\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluatorSGD(Evaluator):\n",
    "    def __init__(self, model, nb_epochs = 50, mini_batch_size=1, lr=1e-4, criterion=LossMSE()):\n",
    "        Evaluator.__init__(self, model)\n",
    "        self.optimizer = SGD(model=model, nb_epochs=nb_epochs, mini_batch_size=mini_batch_size, \n",
    "                             lr=lr, criterion=criterion)\n",
    "        \n",
    "    def cross_validate(self, k=5, values={\"lr\": [1e-5, 1e-4, 1e-3, 1e-2]}):\n",
    "        train_datasets = []\n",
    "        test_datasets = []\n",
    "        for i in range(k):\n",
    "            train_datasets.append(generate_disc_set(1000))\n",
    "            test_datasets.append(generate_disc_set(1000))\n",
    "        \n",
    "        if \"lr\" not in values:\n",
    "            raise ValueError(\"Expected learning rate values to cross-validate...\")\n",
    "        \n",
    "        possible_lrs = values[\"lr\"]\n",
    "        \n",
    "        score_means = []\n",
    "        score_vars = []\n",
    "        for lr in possible_lrs:\n",
    "            print(\"Validating (lr={})\".format(lr))\n",
    "            scores = []\n",
    "            optimizer.lr = lr\n",
    "            \n",
    "            for (train_input, train_target), (test_input, test_target) in zip(train_datasets, test_datasets):\n",
    "                optimizer.model = copy.deepcopy(self.model) \n",
    "                \n",
    "                self.model = optimizer.train(train_input, train_target, verbose=False)\n",
    "                accuracy = self.test(test_input, test_target)\n",
    "                scores.append(accuracy)\n",
    "            \n",
    "            scores = torch.FloatTensor(scores)\n",
    "            score_means.append(torch.mean(scores).item())\n",
    "            score_vars.append(torch.std(scores).item())\n",
    "        best_score = {}\n",
    "        \n",
    "        i = max(enumerate(score_means), key=lambda x: x[1])[0]\n",
    "        \n",
    "        best_score[\"lr\"] = possible_lrs[i]\n",
    "        best_score[\"mean\"] = score_means[i]\n",
    "        best_score[\"std\"] = score_vars[i]\n",
    "        \n",
    "        return dict(zip(possible_lrs, zip(score_means, score_vars))), best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluatorAdam(Evaluator):\n",
    "    def __init__(self, model, nb_epochs = 50, mini_batch_size=1, lr=1e-4, criterion=LossMSE(),\n",
    "                b1=0.9, b2=0.999, epsilon=1e-8):\n",
    "        Evaluator.__init__(self, model)\n",
    "        self.optimizer = Adam(model=model, nb_epochs=nb_epochs, mini_batch_size=mini_batch_size, \n",
    "                              lr=lr, criterion=criterion, b1=b1, b2=b2, epsilon=epsilon)\n",
    "        \n",
    "    def cross_validate(self, k=5, values={\"lr\": [1e-5, 1e-4, 1e-3, 1e-2], \"b1\": [0.9], \n",
    "                                          \"b2\": [0.999], \"epsilon\": [1e-8]}):\n",
    "        train_datasets = []\n",
    "        test_datasets = []\n",
    "        for i in range(k):\n",
    "            train_datasets.append(generate_disc_set(1000))\n",
    "            test_datasets.append(generate_disc_set(1000))\n",
    "        \n",
    "        if \"lr\" not in values or \"b1\" not in values or \"b1\" not in values or \"epsilon\" not in values:\n",
    "            raise ValueError(\"Expected learning rate values to cross-validate...\")\n",
    "            \n",
    "        if \"b1\" not in values:\n",
    "            raise ValueError(\"Expected b1 values to cross-validate...\")\n",
    "\n",
    "        if \"b2\" not in values:\n",
    "            raise ValueError(\"Expected b2 values to cross-validate...\")\n",
    "\n",
    "        if \"epsilon\" not in values:\n",
    "            raise ValueError(\"Expected epsilon values to cross-validate...\")\n",
    "        \n",
    "        lrs = values[\"lr\"]\n",
    "        b1s = values[\"b1\"]\n",
    "        b2s = values[\"b2\"]\n",
    "        epsilons = values[\"epsilon\"]\n",
    "        param_grid = [(lr, b1, b2, epsilon)\n",
    "                        for lr in lrs\n",
    "                        for b1 in b1s\n",
    "                        for b2 in b2s\n",
    "                        for epsilon in epsilons]\n",
    "        \n",
    "        score_means = []\n",
    "        score_vars = []\n",
    "        for (lr, b1, b2, epsilon) in param_grid:\n",
    "            print(\"Validating (lr={}, b1={}, b2={}, epsilon={})...\".format(lr, b1, b2, epsilon))\n",
    "            scores = []\n",
    "            \n",
    "            optimizer.lr = lr\n",
    "            optimizer.b1 = b1\n",
    "            optimizer.b2 = b2\n",
    "            optimizer.epsilon = epsilon\n",
    "                \n",
    "            for (train_input, train_target), (test_input, test_target) in zip(train_datasets, test_datasets):\n",
    "                optimizer.model = copy.deepcopy(self.model) \n",
    "                self.model = optimizer.train(train_input, train_target, verbose=False)\n",
    "                accuracy = self.test(test_input, test_target)\n",
    "                scores.append(accuracy)\n",
    "            \n",
    "            scores = torch.FloatTensor(scores)\n",
    "            score_means.append(torch.mean(scores).item())\n",
    "            score_vars.append(torch.std(scores).item())\n",
    "        best_score = {}\n",
    "        \n",
    "        i = max(enumerate(score_means), key=lambda x: x[1])[0]\n",
    "        \n",
    "        best_score[\"lr\"] = param_grid[i][0]\n",
    "        best_score[\"b1\"] = param_grid[i][1]\n",
    "        best_score[\"b2\"] = param_grid[i][2]\n",
    "        best_score[\"epsilon\"] = param_grid[i][3]\n",
    "        best_score[\"mean\"] = score_means[i]\n",
    "        best_score[\"std\"] = score_vars[i]\n",
    "        \n",
    "        return dict(zip(param_grid, zip(score_means, score_vars))), best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.847140Z",
     "start_time": "2020-04-30T14:39:24.837116Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    input = empty(nb, 2).uniform_(-1, 1)\n",
    "    target = input.pow(2).sum(1).sub(2 / math.pi).sign().add(1).div(2).long()\n",
    "    return input, target\n",
    "\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.856082Z",
     "start_time": "2020-04-30T14:39:24.848087Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = Sequential(Linear(2, 25), ReLU(),\n",
    "#                    Linear(25, 25), ReLU(),\n",
    "#                    Linear(25, 25), ReLU(),\n",
    "#                    Linear(25, 2))\n",
    "# model = Sequential(Linear(2, 25), Tanh(),\n",
    "#                    Linear(25, 25), Tanh(),\n",
    "#                    Linear(25, 25), Tanh(),\n",
    "#                    Linear(25, 2))\n",
    "model = Sequential(Linear(2, 25), Sigmoid(),\n",
    "                   Linear(25, 25), Sigmoid(),\n",
    "                   Linear(25, 25), Sigmoid(),\n",
    "                   Linear(25, 2))\n",
    "\n",
    "evaluator = EvaluatorAdam(model, mini_batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:41:25.712054Z",
     "start_time": "2020-04-30T14:39:24.857063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating (lr=1e-06, b1=0.9, b2=0.999, epsilon=1e-08)...\n",
      "Validating (lr=1e-06, b1=0.9, b2=0.999, epsilon=1e-07)...\n",
      "Validating (lr=1e-06, b1=0.9, b2=0.999, epsilon=1e-06)...\n",
      "Validating (lr=1e-06, b1=0.9, b2=0.888, epsilon=1e-08)...\n",
      "Validating (lr=1e-06, b1=0.9, b2=0.888, epsilon=1e-07)...\n",
      "Validating (lr=1e-06, b1=0.9, b2=0.888, epsilon=1e-06)...\n",
      "Validating (lr=1e-06, b1=0.8, b2=0.999, epsilon=1e-08)...\n",
      "Validating (lr=1e-06, b1=0.8, b2=0.999, epsilon=1e-07)...\n",
      "Validating (lr=1e-06, b1=0.8, b2=0.999, epsilon=1e-06)...\n",
      "Validating (lr=1e-06, b1=0.8, b2=0.888, epsilon=1e-08)...\n",
      "Validating (lr=1e-06, b1=0.8, b2=0.888, epsilon=1e-07)...\n",
      "Validating (lr=1e-06, b1=0.8, b2=0.888, epsilon=1e-06)...\n",
      "Validating (lr=1e-05, b1=0.9, b2=0.999, epsilon=1e-08)...\n",
      "Validating (lr=1e-05, b1=0.9, b2=0.999, epsilon=1e-07)...\n",
      "Validating (lr=1e-05, b1=0.9, b2=0.999, epsilon=1e-06)...\n",
      "Validating (lr=1e-05, b1=0.9, b2=0.888, epsilon=1e-08)...\n",
      "Validating (lr=1e-05, b1=0.9, b2=0.888, epsilon=1e-07)...\n",
      "Validating (lr=1e-05, b1=0.9, b2=0.888, epsilon=1e-06)...\n",
      "Validating (lr=1e-05, b1=0.8, b2=0.999, epsilon=1e-08)...\n",
      "Validating (lr=1e-05, b1=0.8, b2=0.999, epsilon=1e-07)...\n",
      "Validating (lr=1e-05, b1=0.8, b2=0.999, epsilon=1e-06)...\n",
      "Validating (lr=1e-05, b1=0.8, b2=0.888, epsilon=1e-08)...\n",
      "Validating (lr=1e-05, b1=0.8, b2=0.888, epsilon=1e-07)...\n",
      "Validating (lr=1e-05, b1=0.8, b2=0.888, epsilon=1e-06)...\n",
      "Validating (lr=0.0001, b1=0.9, b2=0.999, epsilon=1e-08)...\n",
      "Validating (lr=0.0001, b1=0.9, b2=0.999, epsilon=1e-07)...\n",
      "Validating (lr=0.0001, b1=0.9, b2=0.999, epsilon=1e-06)...\n",
      "Validating (lr=0.0001, b1=0.9, b2=0.888, epsilon=1e-08)...\n",
      "Validating (lr=0.0001, b1=0.9, b2=0.888, epsilon=1e-07)...\n",
      "Validating (lr=0.0001, b1=0.9, b2=0.888, epsilon=1e-06)...\n",
      "Validating (lr=0.0001, b1=0.8, b2=0.999, epsilon=1e-08)...\n",
      "Validating (lr=0.0001, b1=0.8, b2=0.999, epsilon=1e-07)...\n",
      "Validating (lr=0.0001, b1=0.8, b2=0.999, epsilon=1e-06)...\n",
      "Validating (lr=0.0001, b1=0.8, b2=0.888, epsilon=1e-08)...\n",
      "Validating (lr=0.0001, b1=0.8, b2=0.888, epsilon=1e-07)...\n",
      "Validating (lr=0.0001, b1=0.8, b2=0.888, epsilon=1e-06)...\n",
      "Validating (lr=0.001, b1=0.9, b2=0.999, epsilon=1e-08)...\n",
      "Validating (lr=0.001, b1=0.9, b2=0.999, epsilon=1e-07)...\n",
      "Validating (lr=0.001, b1=0.9, b2=0.999, epsilon=1e-06)...\n",
      "Validating (lr=0.001, b1=0.9, b2=0.888, epsilon=1e-08)...\n",
      "Validating (lr=0.001, b1=0.9, b2=0.888, epsilon=1e-07)...\n",
      "Validating (lr=0.001, b1=0.9, b2=0.888, epsilon=1e-06)...\n",
      "Validating (lr=0.001, b1=0.8, b2=0.999, epsilon=1e-08)...\n",
      "Validating (lr=0.001, b1=0.8, b2=0.999, epsilon=1e-07)...\n",
      "Validating (lr=0.001, b1=0.8, b2=0.999, epsilon=1e-06)...\n",
      "Validating (lr=0.001, b1=0.8, b2=0.888, epsilon=1e-08)...\n",
      "Validating (lr=0.001, b1=0.8, b2=0.888, epsilon=1e-07)...\n",
      "Validating (lr=0.001, b1=0.8, b2=0.888, epsilon=1e-06)...\n",
      "Validating (lr=0.01, b1=0.9, b2=0.999, epsilon=1e-08)...\n",
      "Validating (lr=0.01, b1=0.9, b2=0.999, epsilon=1e-07)...\n",
      "Validating (lr=0.01, b1=0.9, b2=0.999, epsilon=1e-06)...\n",
      "Validating (lr=0.01, b1=0.9, b2=0.888, epsilon=1e-08)...\n",
      "Validating (lr=0.01, b1=0.9, b2=0.888, epsilon=1e-07)...\n",
      "Validating (lr=0.01, b1=0.9, b2=0.888, epsilon=1e-06)...\n",
      "Validating (lr=0.01, b1=0.8, b2=0.999, epsilon=1e-08)...\n",
      "Validating (lr=0.01, b1=0.8, b2=0.999, epsilon=1e-07)...\n",
      "Validating (lr=0.01, b1=0.8, b2=0.999, epsilon=1e-06)...\n",
      "Validating (lr=0.01, b1=0.8, b2=0.888, epsilon=1e-08)...\n",
      "Validating (lr=0.01, b1=0.8, b2=0.888, epsilon=1e-07)...\n",
      "Validating (lr=0.01, b1=0.8, b2=0.888, epsilon=1e-06)...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({(1e-06, 0.9, 0.999, 1e-08): (0.39239999651908875, 0.033731285482645035),\n",
       "  (1e-06, 0.9, 0.999, 1e-07): (0.4447999894618988, 0.0225876085460186),\n",
       "  (1e-06, 0.9, 0.999, 1e-06): (0.4668000340461731, 0.02459065429866314),\n",
       "  (1e-06, 0.9, 0.888, 1e-08): (0.5016000270843506, 0.02810337394475937),\n",
       "  (1e-06, 0.9, 0.888, 1e-07): (0.5331999659538269, 0.017584076151251793),\n",
       "  (1e-06, 0.9, 0.888, 1e-06): (0.5635999441146851, 0.01939845085144043),\n",
       "  (1e-06, 0.8, 0.999, 1e-08): (0.5867999792098999, 0.015738470479846),\n",
       "  (1e-06, 0.8, 0.999, 1e-07): (0.61080002784729, 0.015139340423047543),\n",
       "  (1e-06, 0.8, 0.999, 1e-06): (0.6306000351905823, 0.01728583686053753),\n",
       "  (1e-06, 0.8, 0.888, 1e-08): (0.647599995136261, 0.01327779982239008),\n",
       "  (1e-06, 0.8, 0.888, 1e-07): (0.6620000004768372, 0.012980761006474495),\n",
       "  (1e-06, 0.8, 0.888, 1e-06): (0.6808000206947327, 0.012794535607099533),\n",
       "  (1e-05, 0.9, 0.999, 1e-08): (0.8255999684333801, 0.062151435762643814),\n",
       "  (1e-05, 0.9, 0.999, 1e-07): (0.9010000228881836, 0.007713641040027142),\n",
       "  (1e-05, 0.9, 0.999, 1e-06): (0.9149999618530273, 0.00751664862036705),\n",
       "  (1e-05, 0.9, 0.888, 1e-08): (0.9293999671936035, 0.005594633053988218),\n",
       "  (1e-05, 0.9, 0.888, 1e-07): (0.9423999786376953, 0.006465298123657703),\n",
       "  (1e-05, 0.9, 0.888, 1e-06): (0.9526000022888184, 0.009126879274845123),\n",
       "  (1e-05, 0.8, 0.999, 1e-08): (0.9617999792098999, 0.007463240996003151),\n",
       "  (1e-05, 0.8, 0.999, 1e-07): (0.9668000340461731, 0.006058058235794306),\n",
       "  (1e-05, 0.8, 0.999, 1e-06): (0.9682000279426575, 0.005019957199692726),\n",
       "  (1e-05, 0.8, 0.888, 1e-08): (0.9694000482559204, 0.0049295187927782536),\n",
       "  (1e-05, 0.8, 0.888, 1e-07): (0.9697999954223633, 0.005630270577967167),\n",
       "  (1e-05, 0.8, 0.888, 1e-06): (0.9697999954223633, 0.00486826803535223),\n",
       "  (0.0001, 0.9, 0.999, 1e-08): (0.9713999629020691, 0.0056833126582205296),\n",
       "  (0.0001, 0.9, 0.999, 1e-07): (0.9721999168395996, 0.006300788838416338),\n",
       "  (0.0001, 0.9, 0.999, 1e-06): (0.9741999506950378, 0.005167217459529638),\n",
       "  (0.0001, 0.9, 0.888, 1e-08): (0.9746000170707703, 0.006580267567187548),\n",
       "  (0.0001, 0.9, 0.888, 1e-07): (0.9764000177383423, 0.0072663696482777596),\n",
       "  (0.0001, 0.9, 0.888, 1e-06): (0.9750000238418579, 0.0065574427135288715),\n",
       "  (0.0001, 0.8, 0.999, 1e-08): (0.9755999445915222, 0.005458933301270008),\n",
       "  (0.0001, 0.8, 0.999, 1e-07): (0.9758000373840332, 0.005167200695723295),\n",
       "  (0.0001, 0.8, 0.999, 1e-06): (0.9759999513626099, 0.004472141619771719),\n",
       "  (0.0001, 0.8, 0.888, 1e-08): (0.9763998985290527, 0.005079367198050022),\n",
       "  (0.0001, 0.8, 0.888, 1e-07): (0.9765999913215637, 0.0045055425725877285),\n",
       "  (0.0001, 0.8, 0.888, 1e-06): (0.9761999845504761, 0.004868251271545887),\n",
       "  (0.001, 0.9, 0.999, 1e-08): (0.9567999839782715, 0.04186524078249931),\n",
       "  (0.001, 0.9, 0.999, 1e-07): (0.9818000793457031, 0.005890675820410252),\n",
       "  (0.001, 0.9, 0.999, 1e-06): (0.9878000020980835, 0.004381777253001928),\n",
       "  (0.001, 0.9, 0.888, 1e-08): (0.9856001138687134, 0.002701855031773448),\n",
       "  (0.001, 0.9, 0.888, 1e-07): (0.9833999872207642, 0.0019493658328428864),\n",
       "  (0.001, 0.9, 0.888, 1e-06): (0.9803999662399292, 0.0020736446604132652),\n",
       "  (0.001, 0.8, 0.999, 1e-08): (0.9785999059677124, 0.002607674803584814),\n",
       "  (0.001, 0.8, 0.999, 1e-07): (0.9795999526977539, 0.002607674803584814),\n",
       "  (0.001, 0.8, 0.999, 1e-06): (0.9817999601364136, 0.0031937502790242434),\n",
       "  (0.001, 0.8, 0.888, 1e-08): (0.983199954032898, 0.003701350884512067),\n",
       "  (0.001, 0.8, 0.888, 1e-07): (0.9843999743461609, 0.0030495792161673307),\n",
       "  (0.001, 0.8, 0.888, 1e-06): (0.9859999418258667, 0.0032403746154159307),\n",
       "  (0.01, 0.9, 0.999, 1e-08): (0.4896000027656555, 0.012054058723151684),\n",
       "  (0.01, 0.9, 0.999, 1e-07): (0.4896000027656555, 0.012054058723151684),\n",
       "  (0.01, 0.9, 0.999, 1e-06): (0.4896000027656555, 0.012054058723151684),\n",
       "  (0.01, 0.9, 0.888, 1e-08): (0.4896000027656555, 0.012054058723151684),\n",
       "  (0.01, 0.9, 0.888, 1e-07): (0.4896000027656555, 0.012054058723151684),\n",
       "  (0.01, 0.9, 0.888, 1e-06): (0.4896000027656555, 0.012054058723151684),\n",
       "  (0.01, 0.8, 0.999, 1e-08): (0.4896000027656555, 0.012054058723151684),\n",
       "  (0.01, 0.8, 0.999, 1e-07): (0.4896000027656555, 0.012054058723151684),\n",
       "  (0.01, 0.8, 0.999, 1e-06): (0.4896000027656555, 0.012054058723151684),\n",
       "  (0.01, 0.8, 0.888, 1e-08): (0.4896000027656555, 0.012054058723151684),\n",
       "  (0.01, 0.8, 0.888, 1e-07): (0.4896000027656555, 0.012054058723151684),\n",
       "  (0.01, 0.8, 0.888, 1e-06): (0.4896000027656555, 0.012054058723151684)},\n",
       " {'lr': 0.001,\n",
       "  'b1': 0.9,\n",
       "  'b2': 0.999,\n",
       "  'epsilon': 1e-06,\n",
       "  'mean': 0.9878000020980835,\n",
       "  'std': 0.004381777253001928})"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = {\"lr\": [1e-6, 1e-5, 1e-4, 1e-3, 1e-2], \"b1\": [0.9, 0.8], \n",
    "          \"b2\": [0.999, 0.888], \"epsilon\": [1e-8, 1e-7, 1e-6]}\n",
    "evaluator.cross_validate(values=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:41:26.527947Z",
     "start_time": "2020-04-30T14:41:25.713025Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 iteration: loss=8.486946105957031\n",
      "1 iteration: loss=7.5531768798828125\n",
      "2 iteration: loss=6.805228233337402\n",
      "3 iteration: loss=6.143959045410156\n",
      "4 iteration: loss=5.591228485107422\n",
      "5 iteration: loss=5.16232967376709\n",
      "6 iteration: loss=4.821000576019287\n",
      "7 iteration: loss=4.5264739990234375\n",
      "8 iteration: loss=4.251053810119629\n",
      "9 iteration: loss=3.9840495586395264\n",
      "10 iteration: loss=3.7258665561676025\n",
      "11 iteration: loss=3.4747889041900635\n",
      "12 iteration: loss=3.2319607734680176\n",
      "13 iteration: loss=2.998076915740967\n",
      "14 iteration: loss=2.7800989151000977\n",
      "15 iteration: loss=2.576714277267456\n",
      "16 iteration: loss=2.3910794258117676\n",
      "17 iteration: loss=2.222191572189331\n",
      "18 iteration: loss=2.070324659347534\n",
      "19 iteration: loss=1.9336928129196167\n",
      "20 iteration: loss=1.8113350868225098\n",
      "21 iteration: loss=1.7018442153930664\n",
      "22 iteration: loss=1.6020692586898804\n",
      "23 iteration: loss=1.51361882686615\n",
      "24 iteration: loss=1.4326446056365967\n",
      "25 iteration: loss=1.3594814538955688\n",
      "26 iteration: loss=1.2931042909622192\n",
      "27 iteration: loss=1.2322044372558594\n",
      "28 iteration: loss=1.177359938621521\n",
      "29 iteration: loss=1.1238502264022827\n",
      "30 iteration: loss=1.0771088600158691\n",
      "31 iteration: loss=1.0337390899658203\n",
      "32 iteration: loss=0.9938749074935913\n",
      "33 iteration: loss=0.9575982093811035\n",
      "34 iteration: loss=0.9228237867355347\n",
      "35 iteration: loss=0.8898044228553772\n",
      "36 iteration: loss=0.8609046339988708\n",
      "37 iteration: loss=0.8343240022659302\n",
      "38 iteration: loss=0.8080286979675293\n",
      "39 iteration: loss=0.7838789224624634\n",
      "40 iteration: loss=0.7618042230606079\n",
      "41 iteration: loss=0.7383820414543152\n",
      "42 iteration: loss=0.718769371509552\n",
      "43 iteration: loss=0.7009208798408508\n",
      "44 iteration: loss=0.6844415664672852\n",
      "45 iteration: loss=0.6661141514778137\n",
      "46 iteration: loss=0.6512742042541504\n",
      "47 iteration: loss=0.6360682845115662\n",
      "48 iteration: loss=0.6220925450325012\n",
      "49 iteration: loss=0.6091030836105347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9890)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lr = 1e-3\n",
    "model = Sequential(Linear(2, 25), ReLU(),\n",
    "                   Linear(25, 25), Tanh(),\n",
    "                   Linear(25, 25), Sigmoid(),\n",
    "                   Linear(25, 2), xavier_init=False)\n",
    "optimizer = Adam(model, lr=best_lr, mini_batch_size=100, criterion=LossCrossEntropy())\n",
    "model = optimizer.train(train_input, train_target)\n",
    "evaluator.model = model\n",
    "evaluator.test(test_input, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
