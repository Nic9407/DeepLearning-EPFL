{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.716651Z",
     "start_time": "2020-04-30T14:39:24.372938Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import empty, zeros\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.727560Z",
     "start_time": "2020-04-30T14:39:24.717586Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd2d1dc1e50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.733543Z",
     "start_time": "2020-04-30T14:39:24.729555Z"
    }
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def update(self, lr):\n",
    "        pass\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass\n",
    "    \n",
    "    def init_params(self, xavier_init, xavier_gain):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.745550Z",
     "start_time": "2020-04-30T14:39:24.734541Z"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, N_in, N_out, xavier_init=False, xavier_gain=1):\n",
    "        super(Linear, self).__init__()\n",
    "        self.N_in = N_in\n",
    "        self.N_out = N_out\n",
    "        \n",
    "        self.init_params(xavier_init, xavier_gain)\n",
    "        \n",
    "        self.gradW = zeros(self.W.shape)\n",
    "        self.gradb = zeros(self.b.shape)\n",
    "        \n",
    "        self.m_weights = zeros(self.gradW.shape)\n",
    "        self.m_bias = zeros(self.gradb.shape)\n",
    "        \n",
    "        self.v_weights = zeros(self.gradW.shape)\n",
    "        self.v_bias = zeros(self.gradb.shape)\n",
    "    \n",
    "    def init_params(self, xavier_init=True, xavier_gain=1):\n",
    "        if xavier_init:\n",
    "            xavier_std = xavier_gain * math.sqrt(2.0 / (self.N_in + self.N_out))\n",
    "        else:\n",
    "            xavier_std = 1\n",
    "            \n",
    "        self.W = empty((self.N_in, self.N_out)).normal_(0, xavier_std)\n",
    "        self.b = empty((1, self.N_out)).normal_(0, xavier_std)\n",
    "    \n",
    "    def forward(self, *input_):\n",
    "        # out = W * input + b\n",
    "        x = input_[0].clone()\n",
    "        \n",
    "        self.x = x\n",
    "        \n",
    "        return self.x.mm(self.W) + self.b\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # grad_w += input * x^(l-1).t()\n",
    "        # grad_b += input\n",
    "        # out = w.t() * input\n",
    "        # input = grad of activation function, i.e. dl/ds^(l)\n",
    "        # x^(l-1) = input of the forward pass\n",
    "        input_ = gradwrtoutput[0].clone()\n",
    "        \n",
    "        self.gradW += self.x.t().mm(input_)\n",
    "        self.gradb += input_.sum(0)\n",
    "        \n",
    "        return input_.mm(self.W.t())\n",
    "        \n",
    "    def param(self):\n",
    "        return [(self.W, self.gradW, self.m_weights, self.v_weights), (self.b, self.gradb, self.m_bias, self.v_bias)]\n",
    "    \n",
    "    def update(self, lr):\n",
    "        self.W.sub_(lr * self.gradW)\n",
    "        self.b.sub_(lr * self.gradb)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.gradW = zeros(self.W.shape)\n",
    "        self.gradb = zeros(self.b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.755532Z",
     "start_time": "2020-04-30T14:39:24.746509Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self, ):\n",
    "        super(ReLU, self).__init__()\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        s = input_[0].clone()\n",
    "        self.s = s\n",
    "        \n",
    "        s[s < 0] = 0.\n",
    "        \n",
    "        return s\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # out = f'(s^(l)) * input\n",
    "        # s^(l) = input of forward pass\n",
    "        # input = grad of next layer\n",
    "        input_ = gradwrtoutput[0].clone()\n",
    "        \n",
    "        out = self.s.clone()\n",
    "        out[out > 0] = 1\n",
    "        out[out < 0] = 0\n",
    "        \n",
    "        \n",
    "        return out.mul(input_)\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.765459Z",
     "start_time": "2020-04-30T14:39:24.756482Z"
    }
   },
   "outputs": [],
   "source": [
    "class LeakyReLU(Module):\n",
    "    def __init__(self, alpha=0.01):\n",
    "        super(LeakyReLU, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        s = input_[0].clone()\n",
    "        self.s = s\n",
    "        \n",
    "        s[s < 0] = self.alpha * s[s < 0]\n",
    "        \n",
    "        return s\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # out = f'(s^(l)) * input\n",
    "        # s^(l) = input of forward pass\n",
    "        # input = grad of next layer\n",
    "        input_ = gradwrtoutput[0].clone()\n",
    "        \n",
    "        out = self.s.clone()\n",
    "        out[out > 0] = 1\n",
    "        out[out < 0] = self.alpha\n",
    "        \n",
    "        \n",
    "        return out.mul(input_)\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.774467Z",
     "start_time": "2020-04-30T14:39:24.767452Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        s = input_[0].clone()\n",
    "        self.s = s\n",
    "        \n",
    "        return s.tanh()\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # out = f'(s^(l)) * input\n",
    "        # s^(l) = input of forward pass\n",
    "        # input = grad of next layer\n",
    "        input_ = gradwrtoutput[0].clone()\n",
    "        \n",
    "        out = self.s.clone()\n",
    "        out = 1 - out.tanh().pow(2)\n",
    "        \n",
    "        return out.mul(input_)\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.785440Z",
     "start_time": "2020-04-30T14:39:24.775432Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        s = input_[0].clone()\n",
    "        self.s = s\n",
    "        \n",
    "        return s.sigmoid()\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # out = f'(s^(l)) * input\n",
    "        # s^(l) = input of forward pass\n",
    "        # input = grad of next layer\n",
    "        input_ = gradwrtoutput[0].clone()\n",
    "        \n",
    "        out = self.s.clone()\n",
    "        out = out.sigmoid() * (1 - out.sigmoid())\n",
    "        \n",
    "        return out.mul(input_)\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.794418Z",
     "start_time": "2020-04-30T14:39:24.786401Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self, *modules, xavier_init=None, xavier_gain=1):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = modules\n",
    "        self.xavier_init = xavier_init\n",
    "        if xavier_init is not None:\n",
    "            for module in self.modules:\n",
    "                module.init_params(xavier_init, xavier_gain)\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        x = input_[0].clone()\n",
    "        \n",
    "        for m in self.modules:\n",
    "            x = m.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        x = gradwrtoutput[0].clone()\n",
    "        \n",
    "        for i, m in enumerate(reversed(self.modules)):\n",
    "            #print(\"{} : {}\".format(i, x))\n",
    "            x = m.backward(x)\n",
    "        \n",
    "    def param(self):\n",
    "        params = []\n",
    "        \n",
    "        for m in self.modules:\n",
    "            for param in m.param():\n",
    "                params.append(param)\n",
    "        \n",
    "        return params\n",
    "\n",
    "    def update(self, lr):\n",
    "        for m in self.modules:\n",
    "            m.update(lr)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for m in self.modules:\n",
    "            m.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.804520Z",
     "start_time": "2020-04-30T14:39:24.795378Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"class Loss: \n",
    "    ...\n",
    "\"\"\"\n",
    "class LossMSE(Module):\n",
    "    def __init__(self):\n",
    "        super(LossMSE, self).__init__()\n",
    "        pass\n",
    "        \n",
    "    def forward(self, y, target):\n",
    "        # out = e^2\n",
    "        # e = (y - f(x))\n",
    "        self.y = y.clone()\n",
    "        target_onehot = zeros((target.shape[0], 2)) \n",
    "        self.target = target_onehot.scatter_(1, target.view(-1, 1), 1)\n",
    "        \n",
    "        self.e = (self.y - self.target)\n",
    "        self.n = self.y.size(0)\n",
    "        \n",
    "        \n",
    "        return self.e.pow(2).sum()\n",
    "        \n",
    "    def backward(self):\n",
    "        # out = 2 * e\n",
    "        \n",
    "        return 2 * self.e # / self.n        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.815176Z",
     "start_time": "2020-04-30T14:39:24.805199Z"
    }
   },
   "outputs": [],
   "source": [
    "class LossCrossEntropy(Module):\n",
    "    def __init__(self):\n",
    "        super(LossCrossEntropy, self).__init__()\n",
    "        pass\n",
    "        \n",
    "    def forward(self, y, target):\n",
    "        self.y = y.clone()\n",
    "        self.target = target.clone()\n",
    "        sm = torch.softmax(self.y, dim=1)\n",
    "        likelihood = -torch.log(torch.clamp(sm[range(target.size(0)), target], min=1e-3, max=None))\n",
    "        return likelihood.mean()\n",
    "        \n",
    "    def backward(self):\n",
    "        sm = torch.softmax(self.y, dim=1)\n",
    "        target_onehot = zeros((self.target.shape[0], 2)) \n",
    "        target_onehot = target_onehot.scatter_(1, self.target.view(-1, 1), 1)\n",
    "        return sm - target_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.826287Z",
     "start_time": "2020-04-30T14:39:24.816173Z"
    }
   },
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def step(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, model, nb_epochs = 50, mini_batch_size=1, lr=1e-4, criterion=LossMSE()):\n",
    "        super(SGD, self).__init__()\n",
    "        self.model = model\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.lr = lr\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.criterion = criterion\n",
    "    \n",
    "    def step(self):\n",
    "        self.model.update(self.lr)\n",
    "        \n",
    "    # Train function?\n",
    "    def train(self, train_input, train_target, verbose=True):\n",
    "        \n",
    "        for e in range(self.nb_epochs):\n",
    "            sum_loss = 0.\n",
    "        \n",
    "            for b in range(0, train_input.size(0), self.mini_batch_size):\n",
    "                self.model.zero_grad()\n",
    "                output = self.model.forward(train_input.narrow(0, b, self.mini_batch_size))\n",
    "                #print(output)\n",
    "                loss = self.criterion.forward(output, train_target.narrow(0, b, self.mini_batch_size))\n",
    "                # print(loss)\n",
    "            \n",
    "                sum_loss += loss\n",
    "            \n",
    "                l_grad = self.criterion.backward()\n",
    "                self.model.backward(l_grad)\n",
    "                #print(model.param()[0])\n",
    "                self.step()\n",
    "            \n",
    "            #print(model.param()[0])\n",
    "            if verbose:\n",
    "                print(\"{} iteration: loss={}\".format(e, sum_loss))\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, model, nb_epochs = 50, mini_batch_size=1, lr=1e-3, criterion=LossMSE()):\n",
    "        super(Adam, self).__init__()\n",
    "        self.model = model\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.lr = lr\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.criterion = criterion\n",
    "        \n",
    "        self.b1 = 0.9\n",
    "        self.b2 = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "        self.t = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        \n",
    "        for params in self.model.param():\n",
    "            grad = params[1].clone()\n",
    "            m = params[2]\n",
    "            v = params[3]\n",
    "            \n",
    "            m = self.b1 * m + (1 - self.b1) * grad\n",
    "            v = self.b2 * v + (1 - self.b2) * grad.pow(2)\n",
    "            \n",
    "            m_biasc = m / (1 - self.b1 ** self.t)\n",
    "            v_biasc = v / (1 - self.b2 ** self.t)\n",
    "            \n",
    "            params[0].sub_(self.lr * m_biasc/(v_biasc.sqrt() + self.epsilon))\n",
    "        \n",
    "        \n",
    "    # Train function?\n",
    "    def train(self, train_input, train_target, verbose=True):\n",
    "        \n",
    "        for e in range(self.nb_epochs):\n",
    "            sum_loss = 0.\n",
    "        \n",
    "            for b in range(0, train_input.size(0), self.mini_batch_size):\n",
    "                self.model.zero_grad()\n",
    "                output = self.model.forward(train_input.narrow(0, b, self.mini_batch_size))\n",
    "                #print(output)\n",
    "                loss = self.criterion.forward(output, train_target.narrow(0, b, self.mini_batch_size))\n",
    "                # print(loss)\n",
    "            \n",
    "                sum_loss += loss\n",
    "            \n",
    "                l_grad = self.criterion.backward()\n",
    "                self.model.backward(l_grad)\n",
    "                #print(model.param()[0])\n",
    "                self.step()\n",
    "            \n",
    "            #print(model.param()[0])\n",
    "            if verbose:\n",
    "                print(\"{} iteration: loss={}\".format(e, sum_loss))\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.836156Z",
     "start_time": "2020-04-30T14:39:24.827142Z"
    }
   },
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def cross_validate(self, k=5, possible_lrs=[1e-5, 1e-4, 1e-3, 1e-2]):\n",
    "        train_datasets = []\n",
    "        test_datasets = []\n",
    "        for i in range(k):\n",
    "            train_datasets.append(generate_disc_set(1000))\n",
    "            test_datasets.append(generate_disc_set(1000))\n",
    "        scores = {}\n",
    "        score_means = {}\n",
    "        score_vars = {}\n",
    "        for lr in possible_lrs:\n",
    "            print(\"Validating:\", lr)\n",
    "            scores[lr] = []\n",
    "            for (train_input, train_target), (test_input, test_target) in zip(train_datasets, test_datasets):\n",
    "                optimizer.model = self.model \n",
    "                optimizer.lr = lr\n",
    "                self.model = optimizer.train(train_input, train_target, verbose=False)\n",
    "                accuracy = self.test(test_input, test_target)\n",
    "                scores[lr].append(accuracy)\n",
    "            scores[lr] = torch.FloatTensor(scores[lr])\n",
    "            score_means[lr] = torch.mean(scores[lr])\n",
    "            score_vars[lr] = torch.std(scores[lr])\n",
    "            \n",
    "        return score_means, score_vars\n",
    "    \n",
    "    def test(self, test_input, test_target):\n",
    "        num_samples = test_input.size(0)\n",
    "        prediction = self.model.forward(test_input)\n",
    "        predicted_class = torch.argmax(prediction, axis=1)\n",
    "        accuracy = sum(predicted_class == test_target).float() / num_samples\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.847140Z",
     "start_time": "2020-04-30T14:39:24.837116Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    input = empty(nb, 2).uniform_(-1, 1)\n",
    "    target = input.pow(2).sum(1).sub(2 / math.pi).sign().add(1).div(2).long()\n",
    "    return input, target\n",
    "\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.856082Z",
     "start_time": "2020-04-30T14:39:24.848087Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = Sequential(Linear(2, 25), ReLU(),\n",
    "#                    Linear(25, 25), ReLU(),\n",
    "#                    Linear(25, 25), ReLU(),\n",
    "#                    Linear(25, 2))\n",
    "# model = Sequential(Linear(2, 25), Tanh(),\n",
    "#                    Linear(25, 25), Tanh(),\n",
    "#                    Linear(25, 25), Tanh(),\n",
    "#                    Linear(25, 2))\n",
    "model = Sequential(Linear(2, 25), Sigmoid(),\n",
    "                   Linear(25, 25), Sigmoid(),\n",
    "                   Linear(25, 25), Sigmoid(),\n",
    "                   Linear(25, 2))\n",
    "\n",
    "optimizer = SGD(model, mini_batch_size=5)\n",
    "evaluator = Evaluator(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:41:25.712054Z",
     "start_time": "2020-04-30T14:39:24.857063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating: 1e-05\n",
      "Validating: 0.0001\n",
      "Validating: 0.001\n",
      "Validating: 0.01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({1e-05: tensor(0.5828),\n",
       "  0.0001: tensor(0.7502),\n",
       "  0.001: tensor(0.9714),\n",
       "  0.01: tensor(0.9654)},\n",
       " {1e-05: tensor(0.0706),\n",
       "  0.0001: tensor(0.0697),\n",
       "  0.001: tensor(0.0093),\n",
       "  0.01: tensor(0.0234)})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.cross_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:41:26.527947Z",
     "start_time": "2020-04-30T14:41:25.713025Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 iteration: loss=882.33056640625\n",
      "1 iteration: loss=724.82177734375\n",
      "2 iteration: loss=616.5814208984375\n",
      "3 iteration: loss=540.6802368164062\n",
      "4 iteration: loss=489.9297790527344\n",
      "5 iteration: loss=458.3811950683594\n",
      "6 iteration: loss=432.9735412597656\n",
      "7 iteration: loss=407.1817626953125\n",
      "8 iteration: loss=379.7580261230469\n",
      "9 iteration: loss=350.23699951171875\n",
      "10 iteration: loss=318.2288513183594\n",
      "11 iteration: loss=284.6054382324219\n",
      "12 iteration: loss=249.637451171875\n",
      "13 iteration: loss=216.22174072265625\n",
      "14 iteration: loss=187.2932891845703\n",
      "15 iteration: loss=164.9610137939453\n",
      "16 iteration: loss=149.39505004882812\n",
      "17 iteration: loss=140.080322265625\n",
      "18 iteration: loss=133.6435546875\n",
      "19 iteration: loss=129.08587646484375\n",
      "20 iteration: loss=125.69312286376953\n",
      "21 iteration: loss=122.53553009033203\n",
      "22 iteration: loss=119.55784606933594\n",
      "23 iteration: loss=116.46913146972656\n",
      "24 iteration: loss=113.84344482421875\n",
      "25 iteration: loss=111.45858001708984\n",
      "26 iteration: loss=108.47947692871094\n",
      "27 iteration: loss=106.10667419433594\n",
      "28 iteration: loss=103.59465789794922\n",
      "29 iteration: loss=101.1929931640625\n",
      "30 iteration: loss=98.68486785888672\n",
      "31 iteration: loss=96.28317260742188\n",
      "32 iteration: loss=94.09477233886719\n",
      "33 iteration: loss=92.0276107788086\n",
      "34 iteration: loss=89.64696502685547\n",
      "35 iteration: loss=87.66890716552734\n",
      "36 iteration: loss=88.03501892089844\n",
      "37 iteration: loss=85.63088989257812\n",
      "38 iteration: loss=84.51639556884766\n",
      "39 iteration: loss=83.53607940673828\n",
      "40 iteration: loss=81.14346313476562\n",
      "41 iteration: loss=81.5146484375\n",
      "42 iteration: loss=81.01123809814453\n",
      "43 iteration: loss=79.61910247802734\n",
      "44 iteration: loss=77.959716796875\n",
      "45 iteration: loss=77.23993682861328\n",
      "46 iteration: loss=74.8348159790039\n",
      "47 iteration: loss=75.31919860839844\n",
      "48 iteration: loss=74.1908950805664\n",
      "49 iteration: loss=73.00653076171875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9810)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lr = 1e-3\n",
    "model = Sequential(Linear(2, 25), LeakyReLU(),\n",
    "                   Linear(25, 25), LeakyReLU(),\n",
    "                   Linear(25, 25), LeakyReLU(),\n",
    "                   Linear(25, 2), xavier_init=True)\n",
    "optimizer = Adam(model, lr=best_lr, mini_batch_size=100, criterion=LossMSE())\n",
    "model = optimizer.train(train_input, train_target)\n",
    "evaluator.model = model\n",
    "evaluator.test(test_input, test_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
