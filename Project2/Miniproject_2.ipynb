{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.716651Z",
     "start_time": "2020-04-30T14:39:24.372938Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import empty, zeros\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.727560Z",
     "start_time": "2020-04-30T14:39:24.717586Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3cc2317f90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.733543Z",
     "start_time": "2020-04-30T14:39:24.729555Z"
    }
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def update(self, lr):\n",
    "        pass\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass\n",
    "    \n",
    "    def init_params(self, xavier_init, xavier_gain):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.745550Z",
     "start_time": "2020-04-30T14:39:24.734541Z"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, N_in, N_out, xavier_init=False, xavier_gain=1):\n",
    "        Module.__init__(self)\n",
    "        self.N_in = N_in\n",
    "        self.N_out = N_out\n",
    "        \n",
    "        self.init_params(xavier_init, xavier_gain)\n",
    "        \n",
    "        self.gradW = zeros(self.W.shape)\n",
    "        self.gradb = zeros(self.b.shape)\n",
    "        \n",
    "        self.m_weights = zeros(self.gradW.shape)\n",
    "        self.m_bias = zeros(self.gradb.shape)\n",
    "        \n",
    "        self.v_weights = zeros(self.gradW.shape)\n",
    "        self.v_bias = zeros(self.gradb.shape)\n",
    "    \n",
    "    def init_params(self, xavier_init=True, xavier_gain=1):\n",
    "        if xavier_init:\n",
    "            xavier_std = xavier_gain * math.sqrt(2.0 / (self.N_in + self.N_out))\n",
    "        else:\n",
    "            xavier_std = 1\n",
    "            \n",
    "        self.W = empty((self.N_in, self.N_out)).normal_(0, xavier_std)\n",
    "        self.b = empty((1, self.N_out)).normal_(0, xavier_std)\n",
    "    \n",
    "    def forward(self, *input_):\n",
    "        # out = W * input + b\n",
    "        x = input_[0].clone()\n",
    "        \n",
    "        self.x = x\n",
    "        \n",
    "        return self.x.mm(self.W) + self.b\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # grad_w += input * x^(l-1).t()\n",
    "        # grad_b += input\n",
    "        # out = w.t() * input\n",
    "        # input = grad of activation function, i.e. dl/ds^(l)\n",
    "        # x^(l-1) = input of the forward pass\n",
    "        input_ = gradwrtoutput[0].clone()\n",
    "        \n",
    "        self.gradW += self.x.t().mm(input_)\n",
    "        self.gradb += input_.sum(0)\n",
    "        \n",
    "        return input_.mm(self.W.t())\n",
    "        \n",
    "    def param(self):\n",
    "        return [(self.W, self.gradW, self.m_weights, self.v_weights), (self.b, self.gradb, self.m_bias, self.v_bias)]\n",
    "    \n",
    "    def update(self, lr):\n",
    "        self.W.sub_(lr * self.gradW)\n",
    "        self.b.sub_(lr * self.gradb)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.gradW = zeros(self.W.shape)\n",
    "        self.gradb = zeros(self.b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.755532Z",
     "start_time": "2020-04-30T14:39:24.746509Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self, ):\n",
    "        Module.__init__(self)\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        s = input_[0].clone()\n",
    "        self.s = s\n",
    "        \n",
    "        s[s < 0] = 0.\n",
    "        \n",
    "        return s\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # out = f'(s^(l)) * input\n",
    "        # s^(l) = input of forward pass\n",
    "        # input = grad of next layer\n",
    "        input_ = gradwrtoutput[0].clone()\n",
    "        \n",
    "        out = self.s.clone()\n",
    "        out[out > 0] = 1\n",
    "        out[out < 0] = 0\n",
    "        \n",
    "        \n",
    "        return out.mul(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.765459Z",
     "start_time": "2020-04-30T14:39:24.756482Z"
    }
   },
   "outputs": [],
   "source": [
    "class LeakyReLU(Module):\n",
    "    def __init__(self, alpha=0.01):\n",
    "        Module.__init__(self)\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        s = input_[0].clone()\n",
    "        self.s = s\n",
    "        \n",
    "        s[s < 0] = self.alpha * s[s < 0]\n",
    "        \n",
    "        return s\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # out = f'(s^(l)) * input\n",
    "        # s^(l) = input of forward pass\n",
    "        # input = grad of next layer\n",
    "        input_ = gradwrtoutput[0].clone()\n",
    "        \n",
    "        out = self.s.clone()\n",
    "        out[out > 0] = 1\n",
    "        out[out < 0] = self.alpha\n",
    "        \n",
    "        \n",
    "        return out.mul(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.774467Z",
     "start_time": "2020-04-30T14:39:24.767452Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        s = input_[0].clone()\n",
    "        self.s = s\n",
    "        \n",
    "        return s.tanh()\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # out = f'(s^(l)) * input\n",
    "        # s^(l) = input of forward pass\n",
    "        # input = grad of next layer\n",
    "        input_ = gradwrtoutput[0].clone()\n",
    "        \n",
    "        out = self.s.clone()\n",
    "        out = 1 - out.tanh().pow(2)\n",
    "        \n",
    "        return out.mul(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.785440Z",
     "start_time": "2020-04-30T14:39:24.775432Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        s = input_[0].clone()\n",
    "        self.s = s\n",
    "        \n",
    "        return s.sigmoid()\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # out = f'(s^(l)) * input\n",
    "        # s^(l) = input of forward pass\n",
    "        # input = grad of next layer\n",
    "        input_ = gradwrtoutput[0].clone()\n",
    "        \n",
    "        out = self.s.clone()\n",
    "        out = out.sigmoid() * (1 - out.sigmoid())\n",
    "        \n",
    "        return out.mul(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.794418Z",
     "start_time": "2020-04-30T14:39:24.786401Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self, *modules, xavier_init=None, xavier_gain=1):\n",
    "        Module.__init__(self)\n",
    "        self.modules = modules\n",
    "        self.xavier_init = xavier_init\n",
    "        if xavier_init is not None:\n",
    "            for module in self.modules:\n",
    "                module.init_params(xavier_init, xavier_gain)\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        x = input_[0].clone()\n",
    "        \n",
    "        for m in self.modules:\n",
    "            x = m.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        x = gradwrtoutput[0].clone()\n",
    "        \n",
    "        for i, m in enumerate(reversed(self.modules)):\n",
    "            #print(\"{} : {}\".format(i, x))\n",
    "            x = m.backward(x)\n",
    "        \n",
    "    def param(self):\n",
    "        params = []\n",
    "        \n",
    "        for m in self.modules:\n",
    "            for param in m.param():\n",
    "                params.append(param)\n",
    "        \n",
    "        return params\n",
    "\n",
    "    def update(self, lr):\n",
    "        for m in self.modules:\n",
    "            m.update(lr)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for m in self.modules:\n",
    "            m.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.804520Z",
     "start_time": "2020-04-30T14:39:24.795378Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"class Loss: \n",
    "    ...\n",
    "\"\"\"\n",
    "class LossMSE(Module):\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        \n",
    "    def forward(self, y, target):\n",
    "        # out = e^2\n",
    "        # e = (y - f(x))\n",
    "        self.y = y.clone()\n",
    "        target_onehot = zeros((target.shape[0], 2)) \n",
    "        self.target = target_onehot.scatter_(1, target.view(-1, 1), 1)\n",
    "        \n",
    "        self.e = (self.y - self.target)\n",
    "        self.n = self.y.size(0)\n",
    "        \n",
    "        \n",
    "        return self.e.pow(2).sum()\n",
    "        \n",
    "    def backward(self):\n",
    "        # out = 2 * e\n",
    "        \n",
    "        return 2 * self.e # / self.n        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.815176Z",
     "start_time": "2020-04-30T14:39:24.805199Z"
    }
   },
   "outputs": [],
   "source": [
    "class LossCrossEntropy(Module):\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        \n",
    "    def forward(self, y, target):\n",
    "        self.y = y.clone()\n",
    "        self.target = target.clone()\n",
    "        sm = torch.softmax(self.y, dim=1)\n",
    "        likelihood = -torch.log(torch.clamp(sm[range(target.size(0)), target], min=1e-3, max=None))\n",
    "        return likelihood.mean()\n",
    "        \n",
    "    def backward(self):\n",
    "        sm = torch.softmax(self.y, dim=1)\n",
    "        target_onehot = zeros((self.target.shape[0], 2)) \n",
    "        target_onehot = target_onehot.scatter_(1, self.target.view(-1, 1), 1)\n",
    "        return sm - target_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.826287Z",
     "start_time": "2020-04-30T14:39:24.816173Z"
    }
   },
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, model, nb_epochs, mini_batch_size, lr, criterion):\n",
    "        self.model = model\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.lr = lr\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.criterion = criterion\n",
    "    \n",
    "    def train(self, train_input, train_target, verbose=True):\n",
    "        for e in range(self.nb_epochs):\n",
    "            sum_loss = 0.\n",
    "        \n",
    "            for b in range(0, train_input.size(0), self.mini_batch_size):\n",
    "                self.model.zero_grad()\n",
    "                \n",
    "                output = self.model.forward(train_input.narrow(0, b, self.mini_batch_size))\n",
    "                loss = self.criterion.forward(output, train_target.narrow(0, b, self.mini_batch_size))\n",
    "            \n",
    "                sum_loss += loss\n",
    "            \n",
    "                l_grad = self.criterion.backward()\n",
    "                self.model.backward(l_grad)\n",
    "                self.step()\n",
    "                \n",
    "            if verbose:\n",
    "                print(\"{} iteration: loss={}\".format(e, sum_loss))\n",
    "        return self.model\n",
    "    \n",
    "    def step(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, model, nb_epochs = 50, mini_batch_size=1, lr=1e-4, criterion=LossMSE()):\n",
    "        Optimizer.__init__(self, model, nb_epochs, mini_batch_size, lr, criterion)\n",
    "    \n",
    "    def step(self):\n",
    "        self.model.update(self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, model, nb_epochs = 50, mini_batch_size=1, lr=1e-3, criterion=LossMSE(), \n",
    "                 b1=0.9, b2=0.999, epsilon=1e-8):\n",
    "        Optimizer.__init__(self, model, nb_epochs, mini_batch_size, lr, criterion)\n",
    "        \n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        \n",
    "        for (param, grad, m, v) in self.model.param():\n",
    "            g = grad.clone()\n",
    "            \n",
    "            m = self.b1 * m + (1 - self.b1) * g\n",
    "            v = self.b2 * v + (1 - self.b2) * g.pow(2)\n",
    "            \n",
    "            m_biasc = m / (1 - self.b1 ** self.t)\n",
    "            v_biasc = v / (1 - self.b2 ** self.t)\n",
    "            \n",
    "            param.sub_(self.lr * m_biasc/(v_biasc.sqrt() + self.epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.836156Z",
     "start_time": "2020-04-30T14:39:24.827142Z"
    }
   },
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def test(self, test_input, test_target):\n",
    "        num_samples = test_input.size(0)\n",
    "        prediction = self.model.forward(test_input)\n",
    "        self.model.zero_grad()\n",
    "        predicted_class = torch.argmax(prediction, axis=1)\n",
    "        accuracy = sum(predicted_class == test_target).float() / num_samples\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossValidate:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.best_params = None\n",
    "        \n",
    "    def cross_validate(self, k, values, verbose=True):\n",
    "        pass\n",
    "    \n",
    "    def set_params(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, train_input, train_target, verbose=True):\n",
    "        self.optimizer.train(train_input, train_target, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDCV(CrossValidate):\n",
    "    def __init__(self, model, nb_epochs = 50, mini_batch_size=1, lr=1e-4, criterion=LossMSE()):\n",
    "        optimizer = SGD(model=model, nb_epochs=nb_epochs, mini_batch_size=mini_batch_size, \n",
    "                     lr=lr, criterion=criterion)\n",
    "        CrossValidate.__init__(self, model, optimizer)\n",
    "        \n",
    "    def cross_validate(self, k=5, values={\"lr\": [1e-5, 1e-4, 1e-3, 1e-2]}, verbose=True):\n",
    "        train_datasets = []\n",
    "        test_datasets = []\n",
    "        for i in range(k):\n",
    "            train_datasets.append(generate_disc_set(1000))\n",
    "            test_datasets.append(generate_disc_set(1000))\n",
    "        \n",
    "        if \"lr\" not in values:\n",
    "            raise ValueError(\"Expected learning rate values to cross-validate...\")\n",
    "        \n",
    "        possible_lrs = values[\"lr\"]\n",
    "        \n",
    "        score_means = []\n",
    "        score_vars = []\n",
    "        for lr in possible_lrs:\n",
    "            if verbose:\n",
    "                print(\"Validating (lr={})... \".format(lr), end='')\n",
    "            \n",
    "            scores = []\n",
    "            \n",
    "            optim = SGD(model=copy.deepcopy(self.model), nb_epochs=self.optimizer.nb_epochs, mini_batch_size=self.optimizer.mini_batch_size, \n",
    "                     lr=lr, criterion=self.optimizer.criterion)\n",
    "            \n",
    "            for (train_input, train_target), (test_input, test_target) in zip(train_datasets, test_datasets):\n",
    "                optim.model = copy.deepcopy(self.model)\n",
    "                \n",
    "                trained_model = optim.train(train_input, train_target, verbose=False)\n",
    "                \n",
    "                evaluator = Evaluator(optim.model)\n",
    "                accuracy = evaluator.test(test_input, test_target)\n",
    "                \n",
    "                scores.append(accuracy)\n",
    "            \n",
    "            scores = torch.FloatTensor(scores)\n",
    "            scores_mean = torch.mean(scores).item()\n",
    "            scores_var = torch.std(scores).item()\n",
    "            \n",
    "            score_means.append(scores_mean)\n",
    "            score_vars.append(scores_var)\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"Score : {0:.3f} (+/- {1:.3f}) \".format(scores_mean, scores_var))\n",
    "                \n",
    "        best_score = {}\n",
    "        \n",
    "        i = max(enumerate(score_means), key=lambda x: x[1])[0]\n",
    "        \n",
    "        best_score[\"lr\"] = possible_lrs[i]\n",
    "        best_score[\"mean\"] = score_means[i]\n",
    "        best_score[\"std\"] = score_vars[i]\n",
    "        \n",
    "        self.best_params = best_score\n",
    "        \n",
    "        return dict(zip(possible_lrs, zip(score_means, score_vars))), best_score\n",
    "    \n",
    "        \n",
    "    def set_params(self):\n",
    "        if self.best_params is not None:\n",
    "            lr = self.best_params[\"lr\"]\n",
    "            \n",
    "            self.optimizer = SGD(model=self.model, nb_epochs=self.optimizer.nb_epochs, mini_batch_size=self.optimizer.mini_batch_size, \n",
    "                     lr=lr, criterion=self.optimizer.criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamCV(CrossValidate):\n",
    "    def __init__(self, model, nb_epochs = 50, mini_batch_size=1, lr=1e-4, criterion=LossMSE(),\n",
    "                b1=0.9, b2=0.999, epsilon=1e-8):\n",
    "        optimizer = Adam(model=model, nb_epochs=nb_epochs, mini_batch_size=mini_batch_size, \n",
    "                              lr=lr, criterion=criterion, b1=b1, b2=b2, epsilon=epsilon)\n",
    "        CrossValidate.__init__(self, model, optimizer)\n",
    "        \n",
    "    def cross_validate(self, k=5, values={\"lr\": [1e-5, 1e-4, 1e-3, 1e-2], \"b1\": [0.9], \n",
    "                                          \"b2\": [0.999], \"epsilon\": [1e-8]}, verbose=True):\n",
    "        train_datasets = []\n",
    "        test_datasets = []\n",
    "        for i in range(k):\n",
    "            train_datasets.append(generate_disc_set(1000))\n",
    "            test_datasets.append(generate_disc_set(1000))\n",
    "        \n",
    "        if \"lr\" not in values or \"b1\" not in values or \"b1\" not in values or \"epsilon\" not in values:\n",
    "            raise ValueError(\"Expected learning rate values to cross-validate...\")\n",
    "            \n",
    "        if \"b1\" not in values:\n",
    "            raise ValueError(\"Expected b1 values to cross-validate...\")\n",
    "\n",
    "        if \"b2\" not in values:\n",
    "            raise ValueError(\"Expected b2 values to cross-validate...\")\n",
    "\n",
    "        if \"epsilon\" not in values:\n",
    "            raise ValueError(\"Expected epsilon values to cross-validate...\")\n",
    "        \n",
    "        lrs = values[\"lr\"]\n",
    "        b1s = values[\"b1\"]\n",
    "        b2s = values[\"b2\"]\n",
    "        epsilons = values[\"epsilon\"]\n",
    "        param_grid = [(lr, b1, b2, epsilon)\n",
    "                        for lr in lrs\n",
    "                        for b1 in b1s\n",
    "                        for b2 in b2s\n",
    "                        for epsilon in epsilons]\n",
    "        \n",
    "        score_means = []\n",
    "        score_vars = []\n",
    "        for (lr, b1, b2, epsilon) in param_grid:\n",
    "            if verbose:\n",
    "                print(\"Validating (lr={}, b1={}, b2={}, epsilon={})... \".format(lr, b1, b2, epsilon), end='')\n",
    "            \n",
    "            scores = []\n",
    "            \n",
    "            optim = Adam(model=copy.deepcopy(self.model), nb_epochs=self.optimizer.nb_epochs, mini_batch_size=self.optimizer.mini_batch_size, \n",
    "                     lr=lr, criterion=self.optimizer.criterion,  b1=b1, b2=b2, epsilon=epsilon)\n",
    "            \n",
    "            for (train_input, train_target), (test_input, test_target) in zip(train_datasets, test_datasets):\n",
    "                optim.model = copy.deepcopy(self.model)\n",
    "                trained_model = optim.train(train_input, train_target, verbose=False)\n",
    "                \n",
    "                evaluator = Evaluator(optim.model)\n",
    "                accuracy = evaluator.test(test_input, test_target)\n",
    "                \n",
    "                scores.append(accuracy)\n",
    "            \n",
    "            scores = torch.FloatTensor(scores)\n",
    "            scores_mean = torch.mean(scores).item()\n",
    "            scores_var = torch.std(scores).item()\n",
    "            \n",
    "            score_means.append(scores_mean)\n",
    "            score_vars.append(scores_var)\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"Score : {0:.3f} (+/- {1:.3f}) \".format(scores_mean, scores_var))\n",
    "                \n",
    "        best_score = {}\n",
    "        \n",
    "        i = max(enumerate(score_means), key=lambda x: x[1])[0]\n",
    "        \n",
    "        best_score[\"lr\"] = param_grid[i][0]\n",
    "        best_score[\"b1\"] = param_grid[i][1]\n",
    "        best_score[\"b2\"] = param_grid[i][2]\n",
    "        best_score[\"epsilon\"] = param_grid[i][3]\n",
    "        best_score[\"mean\"] = score_means[i]\n",
    "        best_score[\"std\"] = score_vars[i]\n",
    "        \n",
    "        self.best_params = best_score\n",
    "        \n",
    "        return dict(zip(param_grid, zip(score_means, score_vars))), best_score\n",
    "\n",
    "            \n",
    "    def set_params(self):\n",
    "        if self.best_params is not None:\n",
    "            lr = self.best_params[\"lr\"]\n",
    "            b1 = self.best_params[\"b1\"]\n",
    "            b2 = self.best_params[\"b2\"]\n",
    "            epsilon = self.best_params[\"epsilon\"]\n",
    "            \n",
    "            self.optimizer = Adam(model=self.model, nb_epochs=self.optimizer.nb_epochs, mini_batch_size=self.optimizer.mini_batch_size, \n",
    "                     lr=lr, criterion=self.optimizer.criterion,  b1=b1, b2=b2, epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.847140Z",
     "start_time": "2020-04-30T14:39:24.837116Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    input = empty(nb, 2).uniform_(-1, 1)\n",
    "    target = input.pow(2).sum(1).sub(2 / math.pi).sign().add(1).div(2).long()\n",
    "    return input, target\n",
    "\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating (lr=1e-06, b1=0.9, b2=0.999, epsilon=1e-08)... Score : 0.754 (+/- 0.001) \n",
      "Validating (lr=1e-06, b1=0.9, b2=0.999, epsilon=1e-07)... Score : 0.754 (+/- 0.001) \n",
      "Validating (lr=1e-06, b1=0.9, b2=0.999, epsilon=1e-06)... Score : 0.754 (+/- 0.001) \n",
      "Validating (lr=1e-06, b1=0.9, b2=0.888, epsilon=1e-08)... Score : 0.748 (+/- 0.003) \n",
      "Validating (lr=1e-06, b1=0.9, b2=0.888, epsilon=1e-07)... Score : 0.748 (+/- 0.003) \n",
      "Validating (lr=1e-06, b1=0.9, b2=0.888, epsilon=1e-06)... Score : 0.748 (+/- 0.003) \n",
      "Validating (lr=1e-06, b1=0.8, b2=0.999, epsilon=1e-08)... Score : 0.763 (+/- 0.004) \n",
      "Validating (lr=1e-06, b1=0.8, b2=0.999, epsilon=1e-07)... Score : 0.763 (+/- 0.004) \n",
      "Validating (lr=1e-06, b1=0.8, b2=0.999, epsilon=1e-06)... Score : 0.763 (+/- 0.004) \n",
      "Validating (lr=1e-06, b1=0.8, b2=0.888, epsilon=1e-08)... Score : 0.748 (+/- 0.005) \n",
      "Validating (lr=1e-06, b1=0.8, b2=0.888, epsilon=1e-07)... Score : 0.748 (+/- 0.005) \n",
      "Validating (lr=1e-06, b1=0.8, b2=0.888, epsilon=1e-06)... Score : 0.748 (+/- 0.005) \n",
      "Validating (lr=1e-05, b1=0.9, b2=0.999, epsilon=1e-08)... Score : 0.861 (+/- 0.019) \n",
      "Validating (lr=1e-05, b1=0.9, b2=0.999, epsilon=1e-07)... Score : 0.861 (+/- 0.019) \n",
      "Validating (lr=1e-05, b1=0.9, b2=0.999, epsilon=1e-06)... Score : 0.861 (+/- 0.019) \n",
      "Validating (lr=1e-05, b1=0.9, b2=0.888, epsilon=1e-08)... Score : 0.759 (+/- 0.004) \n",
      "Validating (lr=1e-05, b1=0.9, b2=0.888, epsilon=1e-07)... Score : 0.759 (+/- 0.004) \n",
      "Validating (lr=1e-05, b1=0.9, b2=0.888, epsilon=1e-06)... Score : 0.759 (+/- 0.004) \n",
      "Validating (lr=1e-05, b1=0.8, b2=0.999, epsilon=1e-08)... Score : 0.891 (+/- 0.007) \n",
      "Validating (lr=1e-05, b1=0.8, b2=0.999, epsilon=1e-07)... Score : 0.891 (+/- 0.007) \n",
      "Validating (lr=1e-05, b1=0.8, b2=0.999, epsilon=1e-06)... Score : 0.891 (+/- 0.007) \n",
      "Validating (lr=1e-05, b1=0.8, b2=0.888, epsilon=1e-08)... Score : 0.773 (+/- 0.006) \n",
      "Validating (lr=1e-05, b1=0.8, b2=0.888, epsilon=1e-07)... Score : 0.773 (+/- 0.006) \n",
      "Validating (lr=1e-05, b1=0.8, b2=0.888, epsilon=1e-06)... Score : 0.773 (+/- 0.006) \n",
      "Validating (lr=0.0001, b1=0.9, b2=0.999, epsilon=1e-08)... Score : 0.954 (+/- 0.013) \n",
      "Validating (lr=0.0001, b1=0.9, b2=0.999, epsilon=1e-07)... Score : 0.953 (+/- 0.013) \n",
      "Validating (lr=0.0001, b1=0.9, b2=0.999, epsilon=1e-06)... Score : 0.953 (+/- 0.013) \n",
      "Validating (lr=0.0001, b1=0.9, b2=0.888, epsilon=1e-08)... Score : 0.884 (+/- 0.011) \n",
      "Validating (lr=0.0001, b1=0.9, b2=0.888, epsilon=1e-07)... Score : 0.884 (+/- 0.011) \n",
      "Validating (lr=0.0001, b1=0.9, b2=0.888, epsilon=1e-06)... Score : 0.884 (+/- 0.011) \n",
      "Validating (lr=0.0001, b1=0.8, b2=0.999, epsilon=1e-08)... Score : 0.973 (+/- 0.009) \n",
      "Validating (lr=0.0001, b1=0.8, b2=0.999, epsilon=1e-07)... Score : 0.973 (+/- 0.009) \n",
      "Validating (lr=0.0001, b1=0.8, b2=0.999, epsilon=1e-06)... Score : 0.973 (+/- 0.010) \n",
      "Validating (lr=0.0001, b1=0.8, b2=0.888, epsilon=1e-08)... Score : 0.902 (+/- 0.009) \n",
      "Validating (lr=0.0001, b1=0.8, b2=0.888, epsilon=1e-07)... Score : 0.902 (+/- 0.009) \n",
      "Validating (lr=0.0001, b1=0.8, b2=0.888, epsilon=1e-06)... Score : 0.901 (+/- 0.010) \n",
      "Validating (lr=0.001, b1=0.9, b2=0.999, epsilon=1e-08)... Score : 0.953 (+/- 0.020) \n",
      "Validating (lr=0.001, b1=0.9, b2=0.999, epsilon=1e-07)... Score : 0.953 (+/- 0.017) \n",
      "Validating (lr=0.001, b1=0.9, b2=0.999, epsilon=1e-06)... Score : 0.957 (+/- 0.016) \n",
      "Validating (lr=0.001, b1=0.9, b2=0.888, epsilon=1e-08)... Score : 0.971 (+/- 0.010) \n",
      "Validating (lr=0.001, b1=0.9, b2=0.888, epsilon=1e-07)... Score : 0.971 (+/- 0.009) \n",
      "Validating (lr=0.001, b1=0.9, b2=0.888, epsilon=1e-06)... Score : 0.972 (+/- 0.008) \n",
      "Validating (lr=0.001, b1=0.8, b2=0.999, epsilon=1e-08)... Score : 0.936 (+/- 0.022) \n",
      "Validating (lr=0.001, b1=0.8, b2=0.999, epsilon=1e-07)... Score : 0.927 (+/- 0.026) \n",
      "Validating (lr=0.001, b1=0.8, b2=0.999, epsilon=1e-06)... Score : 0.934 (+/- 0.023) \n",
      "Validating (lr=0.001, b1=0.8, b2=0.888, epsilon=1e-08)... Score : 0.979 (+/- 0.013) \n",
      "Validating (lr=0.001, b1=0.8, b2=0.888, epsilon=1e-07)... Score : 0.978 (+/- 0.012) \n",
      "Validating (lr=0.001, b1=0.8, b2=0.888, epsilon=1e-06)... Score : 0.976 (+/- 0.012) \n",
      "Validating (lr=0.01, b1=0.9, b2=0.999, epsilon=1e-08)... Score : 0.870 (+/- 0.039) \n",
      "Validating (lr=0.01, b1=0.9, b2=0.999, epsilon=1e-07)... Score : 0.888 (+/- 0.027) \n",
      "Validating (lr=0.01, b1=0.9, b2=0.999, epsilon=1e-06)... Score : 0.892 (+/- 0.023) \n",
      "Validating (lr=0.01, b1=0.9, b2=0.888, epsilon=1e-08)... Score : 0.955 (+/- 0.008) \n",
      "Validating (lr=0.01, b1=0.9, b2=0.888, epsilon=1e-07)... Score : 0.956 (+/- 0.014) \n",
      "Validating (lr=0.01, b1=0.9, b2=0.888, epsilon=1e-06)... Score : 0.960 (+/- 0.016) \n",
      "Validating (lr=0.01, b1=0.8, b2=0.999, epsilon=1e-08)... Score : 0.853 (+/- 0.044) \n",
      "Validating (lr=0.01, b1=0.8, b2=0.999, epsilon=1e-07)... Score : 0.837 (+/- 0.040) \n",
      "Validating (lr=0.01, b1=0.8, b2=0.999, epsilon=1e-06)... Score : 0.852 (+/- 0.035) \n",
      "Validating (lr=0.01, b1=0.8, b2=0.888, epsilon=1e-08)... Score : 0.930 (+/- 0.021) \n",
      "Validating (lr=0.01, b1=0.8, b2=0.888, epsilon=1e-07)... Score : 0.928 (+/- 0.020) \n",
      "Validating (lr=0.01, b1=0.8, b2=0.888, epsilon=1e-06)... Score : 0.925 (+/- 0.014) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9810)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential(Linear(2, 25), ReLU(),\n",
    "                   Linear(25, 25), ReLU(),\n",
    "                   Linear(25, 25), ReLU(),\n",
    "                   Linear(25, 2))\n",
    "\n",
    "optimizer = AdamCV(model, mini_batch_size=100, criterion=LossCrossEntropy())\n",
    "\n",
    "values = {\"lr\": [1e-6, 1e-5, 1e-4, 1e-3, 1e-2], \"b1\": [0.9, 0.8], \n",
    "          \"b2\": [0.999, 0.888], \"epsilon\": [1e-8, 1e-7, 1e-6]}\n",
    "\n",
    "optimizer.cross_validate(k=3, values=values)\n",
    "\n",
    "optimizer.set_params()\n",
    "\n",
    "optimizer.train(train_input, train_target, verbose=False)\n",
    "\n",
    "evaluator = Evaluator(model)\n",
    "\n",
    "evaluator.test(test_input, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating (lr=1e-06)... Score : 0.844 (+/- 0.011) \n",
      "Validating (lr=1e-05)... Score : 0.969 (+/- 0.008) \n",
      "Validating (lr=0.0001)... Score : 0.981 (+/- 0.003) \n",
      "Validating (lr=0.001)... Score : 0.972 (+/- 0.005) \n",
      "Validating (lr=0.01)... Score : 0.514 (+/- 0.013) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9710)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential(Linear(2, 25), ReLU(),\n",
    "                   Linear(25, 25), ReLU(),\n",
    "                   Linear(25, 25), ReLU(),\n",
    "                   Linear(25, 2))\n",
    "\n",
    "optimizer = SGDCV(model, mini_batch_size=100, criterion=LossCrossEntropy())\n",
    "\n",
    "values = {\"lr\": [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]}\n",
    "\n",
    "optimizer.cross_validate(k=3, values=values)\n",
    "\n",
    "optimizer.set_params()\n",
    "\n",
    "optimizer.train(train_input, train_target, verbose=False)\n",
    "\n",
    "evaluator = Evaluator(model)\n",
    "\n",
    "evaluator.test(test_input, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:39:24.856082Z",
     "start_time": "2020-04-30T14:39:24.848087Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = Sequential(Linear(2, 25), ReLU(),\n",
    "#                    Linear(25, 25), ReLU(),\n",
    "#                    Linear(25, 25), ReLU(),\n",
    "#                    Linear(25, 2))\n",
    "# model = Sequential(Linear(2, 25), Tanh(),\n",
    "#                    Linear(25, 25), Tanh(),\n",
    "#                    Linear(25, 25), Tanh(),\n",
    "#                    Linear(25, 2))\n",
    "model = Sequential(Linear(2, 25), Sigmoid(),\n",
    "                   Linear(25, 25), Sigmoid(),\n",
    "                   Linear(25, 25), Sigmoid(),\n",
    "                   Linear(25, 2))\n",
    "\n",
    "evaluator = EvaluatorAdam(model, mini_batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:41:25.712054Z",
     "start_time": "2020-04-30T14:39:24.857063Z"
    }
   },
   "outputs": [],
   "source": [
    "values = {\"lr\": [1e-6, 1e-5, 1e-4, 1e-3, 1e-2], \"b1\": [0.9, 0.8], \n",
    "          \"b2\": [0.999, 0.888], \"epsilon\": [1e-8, 1e-7, 1e-6]}\n",
    "evaluator.cross_validate(values=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T14:41:26.527947Z",
     "start_time": "2020-04-30T14:41:25.713025Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_lr = 1e-3\n",
    "model = Sequential(Linear(2, 25), ReLU(),\n",
    "                   Linear(25, 25), Tanh(),\n",
    "                   Linear(25, 25), Sigmoid(),\n",
    "                   Linear(25, 2), xavier_init=False)\n",
    "optimizer = Adam(model, lr=best_lr, mini_batch_size=100, criterion=LossCrossEntropy())\n",
    "model = optimizer.train(train_input, train_target)\n",
    "evaluator.model = model\n",
    "evaluator.test(test_input, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
