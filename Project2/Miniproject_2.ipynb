{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import empty, zeros\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def update(self, lr):\n",
    "        pass\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, N_in, N_out):\n",
    "        super(Linear, self).__init__()\n",
    "        self.N_in = N_in\n",
    "        self.N_out = N_out\n",
    "        \n",
    "        self.W = empty((N_in, N_out)).normal_()\n",
    "        self.b = empty((1, N_out)).normal_()\n",
    "        \n",
    "        self.gradW = zeros(self.W.shape).normal_()\n",
    "        self.gradb = zeros(self.b.shape).normal_()\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        # out = W * input + b\n",
    "        x = input_[0].clone()\n",
    "        \n",
    "        self.x = x\n",
    "        \n",
    "        return self.x.mm(self.W) + self.b\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # grad_w += input * x^(l-1).t()\n",
    "        # grad_b += input\n",
    "        # out = w.t() * input\n",
    "        # input = grad of activation function, i.e. dl/ds^(l)\n",
    "        # x^(l-1) = input of the forward pass\n",
    "        input_ = gradwrtoutput[0].clone()\n",
    "        \n",
    "        self.gradW += self.x.t().mm(input_)\n",
    "        self.gradb += input_.sum(0)\n",
    "        \n",
    "        return input_.mm(self.W.t())\n",
    "        \n",
    "    def param(self):\n",
    "        return [(self.W, self.gradW), (self.b, self.gradb)]\n",
    "    \n",
    "    def update(self, lr):\n",
    "        self.W.sub_(lr * self.gradW)\n",
    "        self.b.sub_(lr * self.gradb)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.gradW = zeros(self.W.shape)\n",
    "        self.gradb = zeros(self.b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        s = input_[0].clone()\n",
    "        self.s = s\n",
    "        \n",
    "        s[s < 0] = 0.\n",
    "        \n",
    "        return s\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # out = f'(s^(l)) * input\n",
    "        # s^(l) = input of forward pass\n",
    "        # input = grad of next layer\n",
    "        input_ = gradwrtoutput[0].clone()\n",
    "        \n",
    "        out = self.s.clone()\n",
    "        out[out > 0] = 1\n",
    "        out[out < 0] = 0\n",
    "        \n",
    "        \n",
    "        return out.mul(input_)\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        s = input_[0].clone()\n",
    "        self.s = s\n",
    "        \n",
    "        return s.tanh()\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # out = f'(s^(l)) * input\n",
    "        # s^(l) = input of forward pass\n",
    "        # input = grad of next layer\n",
    "        input_ = gradwrtoutput[0].clone()\n",
    "        \n",
    "        out = self.s.clone()\n",
    "        out = 1 - out.tanh().pow(2)\n",
    "        \n",
    "        return out.mul(input_)\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self, *modules):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = modules\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        x = input_[0].clone()\n",
    "        \n",
    "        for m in self.modules:\n",
    "            x = m.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        x = gradwrtoutput[0].clone()\n",
    "        \n",
    "        for i, m in enumerate(reversed(self.modules)):\n",
    "            #print(\"{} : {}\".format(i, x))\n",
    "            x = m.backward(x)\n",
    "        \n",
    "    def param(self):\n",
    "        params = []\n",
    "        \n",
    "        for m in self.modules:\n",
    "            for param in m.param():\n",
    "                params.append(param)\n",
    "        \n",
    "        return params\n",
    "\n",
    "    def update(self, lr):\n",
    "        for m in self.modules:\n",
    "            m.update(lr)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for m in self.modules:\n",
    "            m.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class Loss: \n",
    "    ...\n",
    "\"\"\"\n",
    "class LossMSE(Module):\n",
    "    def __init__(self):\n",
    "        super(LossMSE, self).__init__()\n",
    "        pass\n",
    "        \n",
    "    def forward(self, y, target):\n",
    "        # out = e^2\n",
    "        # e = (y - f(x))\n",
    "        self.y = y.clone()\n",
    "        target_onehot = zeros((target.shape[0], 2)) \n",
    "        self.target = target_onehot.scatter_(1, target.view(-1, 1), 1)\n",
    "        \n",
    "        self.e = (self.y - self.target)\n",
    "        \n",
    "        \n",
    "        return self.e.pow(2).sum()\n",
    "        \n",
    "    def backward(self):\n",
    "        # out = 2 * e\n",
    "        \n",
    "        return 2 * self.e\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def step(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, model, lr):\n",
    "        super(SGD, self).__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "    \n",
    "    # Train function?\n",
    "    # def train(..):\n",
    "    \n",
    "    def step(self):\n",
    "        model.update(self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    input = empty(nb, 2).uniform_(-1, 1)\n",
    "    target = input.pow(2).sum(1).sub(2 / math.pi).sign().add(1).div(2).long()\n",
    "    return input, target\n",
    "\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4521,  0.0235],\n",
       "        [-0.0999,  0.8495],\n",
       "        [-0.4522, -0.3818],\n",
       "        ...,\n",
       "        [-0.6159, -0.8177],\n",
       "        [-0.2290,  0.1896],\n",
       "        [-0.9179,  0.2292]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, mini_batch_size=1):\n",
    "    criterion = LossMSE()\n",
    "    optimizer = SGD(model, lr = 1e-4)\n",
    "    nb_epochs = 50\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "        sum_loss = 0.\n",
    "        \n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            model.zero_grad()\n",
    "            output = model.forward(train_input.narrow(0, b, mini_batch_size))\n",
    "            #print(output)\n",
    "            loss = criterion.forward(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            \n",
    "            sum_loss += loss\n",
    "            \n",
    "            l_grad = criterion.backward()\n",
    "            model.backward(l_grad)\n",
    "            #print(model.param()[0])\n",
    "            optimizer.step()\n",
    "            \n",
    "        #print(model.param()[0])\n",
    "            \n",
    "        print(\"{} iteration: loss={}\".format(e, sum_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 iteration: loss=103039.765625\n",
      "1 iteration: loss=1267.904541015625\n",
      "2 iteration: loss=942.134521484375\n",
      "3 iteration: loss=793.8841552734375\n",
      "4 iteration: loss=696.9398803710938\n",
      "5 iteration: loss=624.7383422851562\n",
      "6 iteration: loss=568.2059936523438\n",
      "7 iteration: loss=523.1644287109375\n",
      "8 iteration: loss=486.41412353515625\n",
      "9 iteration: loss=454.4186706542969\n",
      "10 iteration: loss=427.0074157714844\n",
      "11 iteration: loss=404.56329345703125\n",
      "12 iteration: loss=384.3256530761719\n",
      "13 iteration: loss=365.3061218261719\n",
      "14 iteration: loss=349.16021728515625\n",
      "15 iteration: loss=335.129150390625\n",
      "16 iteration: loss=322.6468811035156\n",
      "17 iteration: loss=311.48321533203125\n",
      "18 iteration: loss=301.76947021484375\n",
      "19 iteration: loss=291.84222412109375\n",
      "20 iteration: loss=284.13641357421875\n",
      "21 iteration: loss=276.53497314453125\n",
      "22 iteration: loss=269.75921630859375\n",
      "23 iteration: loss=263.9503173828125\n",
      "24 iteration: loss=258.34381103515625\n",
      "25 iteration: loss=254.33367919921875\n",
      "26 iteration: loss=249.36134338378906\n",
      "27 iteration: loss=244.66017150878906\n",
      "28 iteration: loss=240.73004150390625\n",
      "29 iteration: loss=236.46849060058594\n",
      "30 iteration: loss=232.85986328125\n",
      "31 iteration: loss=229.54148864746094\n",
      "32 iteration: loss=226.36068725585938\n",
      "33 iteration: loss=223.47201538085938\n",
      "34 iteration: loss=220.37806701660156\n",
      "35 iteration: loss=217.57113647460938\n",
      "36 iteration: loss=215.1222381591797\n",
      "37 iteration: loss=212.82994079589844\n",
      "38 iteration: loss=210.66229248046875\n",
      "39 iteration: loss=208.6533966064453\n",
      "40 iteration: loss=207.20672607421875\n",
      "41 iteration: loss=205.29913330078125\n",
      "42 iteration: loss=203.58804321289062\n",
      "43 iteration: loss=202.04991149902344\n",
      "44 iteration: loss=200.46946716308594\n",
      "45 iteration: loss=198.90716552734375\n",
      "46 iteration: loss=197.50059509277344\n",
      "47 iteration: loss=195.8454132080078\n",
      "48 iteration: loss=194.59449768066406\n",
      "49 iteration: loss=193.18040466308594\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(Linear(2, 25), ReLU(),\n",
    "                   Linear(25, 25), ReLU(),\n",
    "                   Linear(25, 25), ReLU(),\n",
    "                   Linear(25, 2))\n",
    "\n",
    "\n",
    "train_model(model, train_input, train_target, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def test(model, test_input, test_target):\n",
    "    num_samples = test_input.size(0)\n",
    "    prediction = model.forward(test_input)\n",
    "    predicted_class = torch.argmax(prediction, axis=1)\n",
    "    accuracy = sum(predicted_class == test_target).float() / num_samples\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9220)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, test_input, test_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
