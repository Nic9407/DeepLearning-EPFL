{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import empty, zeros\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def update(self, lr):\n",
    "        pass\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, N_in, N_out):\n",
    "        super(Linear, self).__init__()\n",
    "        self.N_in = N_in\n",
    "        self.N_out = N_out\n",
    "        \n",
    "        self.W = empty((N_out, N_in)).normal_()\n",
    "        self.b = empty(N_out).normal_()\n",
    "        \n",
    "        self.gradW = zeros((N_out, N_in)).normal_()\n",
    "        self.gradb = zeros(1, N_out).normal_()\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        # out = W * input + b\n",
    "        x = input_[0].clone()\n",
    "        \n",
    "        self.x = x.clone()\n",
    "        \n",
    "        return self.W.mm(x.t()).t() + self.b\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # grad_w += input * x^(l-1).t()\n",
    "        # grad_b += input\n",
    "        # out = w.t() * input\n",
    "        # input = grad of activation function, i.e. dl/ds^(l)\n",
    "        # x^(l-1) = input of the forward pass\n",
    "        input_ = gradwrtoutput[0].clone()\n",
    "        \n",
    "        self.gradW = self.gradW.clone() + input_.t().mm(self.x)\n",
    "        self.gradb = self.gradb.clone() + input_\n",
    "        \n",
    "        return input_.mm(self.W)\n",
    "        \n",
    "    def param(self):\n",
    "        return [(self.W, self.gradW), (self.b.view(1, -1), self.gradb)]\n",
    "    \n",
    "    def update(self, lr):\n",
    "        self.W.sub_(lr * self.gradW)\n",
    "        self.b.view(1, -1).sub_(lr * self.gradb)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.gradW = zeros((self.N_out, self.N_in))\n",
    "        self.gradb = zeros(1, self.N_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        s = input_[0].clone()\n",
    "        self.s = s.clone()\n",
    "        s[s < 0] = 0.\n",
    "        \n",
    "        return s\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # out = f'(s^(l)) * input\n",
    "        # s^(l) = input of forward pass\n",
    "        # input = grad of next layer\n",
    "        input_ = gradwrtoutput[0].clone()\n",
    "        \n",
    "        out = self.s.clone()\n",
    "        out[out > 0] = 1\n",
    "        out[out < 0] = 0\n",
    "        \n",
    "        \n",
    "        return out * input_\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        s = input_[0]\n",
    "        self.s = s.clone()\n",
    "        \n",
    "        return s.tanh()\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # out = f'(s^(l)) * input\n",
    "        # s^(l) = input of forward pass\n",
    "        # input = grad of next layer\n",
    "        input_ = gradwrtoutput[0]\n",
    "        out = self.s\n",
    "        out = 1 - out.tanh().pow(2)\n",
    "        \n",
    "        return out * input_\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self, *modules):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = modules\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        x = input_[0].clone()\n",
    "        \n",
    "        for m in self.modules:\n",
    "            x = m.forward(x.clone()).clone()\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        x = gradwrtoutput[0].clone()\n",
    "        \n",
    "        for i, m in enumerate(reversed(self.modules)):\n",
    "            #print(\"{} : {}\".format(i, x))\n",
    "            x = m.backward(x.clone()).clone()     \n",
    "        \n",
    "    def param(self):\n",
    "        params = []\n",
    "        \n",
    "        for m in self.modules:\n",
    "            for param in m.param():\n",
    "                params.append(param)\n",
    "        \n",
    "        return params\n",
    "\n",
    "    def update(self, lr):\n",
    "        for m in self.modules:\n",
    "            m.update(lr)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for m in self.modules:\n",
    "            m.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class Loss: \n",
    "    ...\n",
    "\"\"\"\n",
    "class LossMSE(Module):\n",
    "    def __init__(self):\n",
    "        super(LossMSE, self).__init__()\n",
    "        pass\n",
    "        \n",
    "    def forward(self, y, target):\n",
    "        # out = e^2\n",
    "        # e = (y - f(x))\n",
    "        \n",
    "        self.y = y.clone()\n",
    "        target_onehot = zeros((target.shape[0], 2)) \n",
    "        self.target = target_onehot.scatter_(1, target.view(-1, 1), 1)\n",
    "        \n",
    "        e = (self.y - self.target)\n",
    "        \n",
    "        \n",
    "        return e.pow(2).sum()\n",
    "        \n",
    "    def backward(self):\n",
    "        # out = 2 * e\n",
    "        \n",
    "        e = (self.y - self.target)\n",
    "        \n",
    "        return 2 * e\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def step(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, model, lr):\n",
    "        super(SGD, self).__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "    \n",
    "    # Train function?\n",
    "    # def train(..):\n",
    "    \n",
    "    def step(self):\n",
    "        model.update(self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    input = empty(nb, 2).uniform_(-1, 1)\n",
    "    target = input.pow(2).sum(1).sub(2 / math.pi).sign().add(1).div(2).long()\n",
    "    return input, target\n",
    "\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9012, -0.3991],\n",
       "        [ 0.7823, -0.7173],\n",
       "        [ 0.6446,  0.3661],\n",
       "        ...,\n",
       "        [ 0.2330,  0.2137],\n",
       "        [ 0.9048, -0.5823],\n",
       "        [ 0.3929, -0.9313]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, mini_batch_size=1):\n",
    "    criterion = LossMSE()\n",
    "    optimizer = SGD(model, lr = 1e-4)\n",
    "    nb_epochs = 50\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "        sum_loss = 0.\n",
    "        \n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            model.zero_grad()\n",
    "            output = model.forward(train_input.narrow(0, b, mini_batch_size))\n",
    "            #print(output)\n",
    "            loss = criterion.forward(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            \n",
    "            sum_loss += loss\n",
    "            \n",
    "            l_grad = criterion.backward()\n",
    "            model.backward(l_grad)\n",
    "            #print(model.param()[0])\n",
    "            optimizer.step()\n",
    "            \n",
    "        #print(model.param()[0])\n",
    "            \n",
    "        print(\"{} iteration: loss={}\".format(e, sum_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 iteration: loss=40971.203125\n",
      "1 iteration: loss=1617.0498046875\n",
      "2 iteration: loss=1122.658935546875\n",
      "3 iteration: loss=928.7864379882812\n",
      "4 iteration: loss=771.3236083984375\n",
      "5 iteration: loss=678.635009765625\n",
      "6 iteration: loss=603.58837890625\n",
      "7 iteration: loss=545.9110107421875\n",
      "8 iteration: loss=496.76922607421875\n",
      "9 iteration: loss=457.56561279296875\n",
      "10 iteration: loss=425.74481201171875\n",
      "11 iteration: loss=401.3797912597656\n",
      "12 iteration: loss=381.05694580078125\n",
      "13 iteration: loss=363.7950439453125\n",
      "14 iteration: loss=348.4145812988281\n",
      "15 iteration: loss=336.3956604003906\n",
      "16 iteration: loss=324.5934753417969\n",
      "17 iteration: loss=314.39886474609375\n",
      "18 iteration: loss=304.3576965332031\n",
      "19 iteration: loss=295.8401184082031\n",
      "20 iteration: loss=286.9771423339844\n",
      "21 iteration: loss=279.5616760253906\n",
      "22 iteration: loss=272.9071350097656\n",
      "23 iteration: loss=265.51904296875\n",
      "24 iteration: loss=259.39361572265625\n",
      "25 iteration: loss=253.3102264404297\n",
      "26 iteration: loss=247.75823974609375\n",
      "27 iteration: loss=243.04310607910156\n",
      "28 iteration: loss=237.5321502685547\n",
      "29 iteration: loss=233.3750762939453\n",
      "30 iteration: loss=228.8336944580078\n",
      "31 iteration: loss=224.6096954345703\n",
      "32 iteration: loss=220.1334686279297\n",
      "33 iteration: loss=216.48744201660156\n",
      "34 iteration: loss=213.00912475585938\n",
      "35 iteration: loss=210.05532836914062\n",
      "36 iteration: loss=206.75462341308594\n",
      "37 iteration: loss=204.1066131591797\n",
      "38 iteration: loss=201.9628448486328\n",
      "39 iteration: loss=199.18629455566406\n",
      "40 iteration: loss=196.62890625\n",
      "41 iteration: loss=194.45680236816406\n",
      "42 iteration: loss=192.4310302734375\n",
      "43 iteration: loss=190.3658447265625\n",
      "44 iteration: loss=188.52325439453125\n",
      "45 iteration: loss=186.6293487548828\n",
      "46 iteration: loss=184.87722778320312\n",
      "47 iteration: loss=183.4779052734375\n",
      "48 iteration: loss=181.8448944091797\n",
      "49 iteration: loss=180.3291778564453\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(Linear(2, 25), ReLU(),\n",
    "                   Linear(25, 25), ReLU(),\n",
    "                   Linear(25, 25), ReLU(),\n",
    "                   Linear(25, 2))\n",
    "\n",
    "\n",
    "train_model(model, train_input, train_target, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def test(model, test_input, test_target):\n",
    "    num_samples = test_input.size(0)\n",
    "    prediction = model.forward(test_input)\n",
    "    predicted_class = torch.argmax(prediction, axis=1)\n",
    "    accuracy = sum(predicted_class == test_target).float() / num_samples\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9260)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, test_input, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
