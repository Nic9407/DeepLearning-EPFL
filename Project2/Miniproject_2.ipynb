{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import empty\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def update(self, lr):\n",
    "        pass\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, N_in, N_out):\n",
    "        super(Linear, self).__init__()\n",
    "        self.N_in = N_in\n",
    "        self.N_out = N_out\n",
    "        \n",
    "        self.W = empty((N_out, N_in)).normal_()\n",
    "        self.b = empty(N_out).normal_()\n",
    "        \n",
    "        self.gradW = torch.zeros((N_out, N_in)).normal_()\n",
    "        self.gradb = torch.zeros(1, N_out).normal_()\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        # out = W * input + b\n",
    "        x = input_[0].clone()\n",
    "        \n",
    "        self.x = x.clone()\n",
    "        \n",
    "        return self.W.mm(x.t()).t() + self.b\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # grad_w += input * x^(l-1).t()\n",
    "        # grad_b += input\n",
    "        # out = w.t() * input\n",
    "        # input = grad of activation function, i.e. dl/ds^(l)\n",
    "        # x^(l-1) = input of the forward pass\n",
    "        input_ = gradwrtoutput[0].clone()\n",
    "        \n",
    "        self.gradW = self.gradW.clone() + input_.t().mm(self.x)\n",
    "        self.gradb = self.gradb.clone() + input_\n",
    "        \n",
    "        return input_.mm(self.W)\n",
    "        \n",
    "    def param(self):\n",
    "        return [(self.W, self.gradW), (self.b.view(1, -1), self.gradb)]\n",
    "    \n",
    "    def update(self, lr):\n",
    "        self.W.sub_(lr * self.gradW)\n",
    "        self.b.view(1, -1).sub_(lr * self.gradb)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.gradW = torch.zeros((self.N_out, self.N_in))\n",
    "        self.gradb = torch.zeros(1, self.N_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        s = input_[0].clone()\n",
    "        self.s = s.clone()\n",
    "        s[s < 0] = 0.\n",
    "        \n",
    "        return s\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # out = f'(s^(l)) * input\n",
    "        # s^(l) = input of forward pass\n",
    "        # input = grad of next layer\n",
    "        input_ = gradwrtoutput[0].clone()\n",
    "        \n",
    "        out = self.s.clone()\n",
    "        out[out > 0] = 1\n",
    "        out[out < 0] = 0\n",
    "        \n",
    "        \n",
    "        return out * input_\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        s = input_[0]\n",
    "        self.s = s.clone()\n",
    "        \n",
    "        return s.tanh()\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        # out = f'(s^(l)) * input\n",
    "        # s^(l) = input of forward pass\n",
    "        # input = grad of next layer\n",
    "        input_ = gradwrtoutput[0]\n",
    "        out = self.s\n",
    "        out = 1 - out.tanh().pow(2)\n",
    "        \n",
    "        return out * input_\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self, *modules):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = modules\n",
    "        \n",
    "    def forward(self, *input_):\n",
    "        x = input_[0].clone()\n",
    "        \n",
    "        for m in self.modules:\n",
    "            x = m.forward(x.clone()).clone()\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        x = gradwrtoutput[0].clone()\n",
    "        \n",
    "        for i, m in enumerate(reversed(self.modules)):\n",
    "            #print(\"{} : {}\".format(i, x))\n",
    "            x = m.backward(x.clone()).clone()     \n",
    "        \n",
    "    def param(self):\n",
    "        params = []\n",
    "        \n",
    "        for m in self.modules:\n",
    "            for param in m.param():\n",
    "                params.append(param)\n",
    "        \n",
    "        return params\n",
    "\n",
    "    def update(self, lr):\n",
    "        for m in self.modules:\n",
    "            m.update(lr)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for m in self.modules:\n",
    "            m.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class Loss: \n",
    "    ...\n",
    "\"\"\"\n",
    "class LossMSE(Module):\n",
    "    def __init__(self):\n",
    "        super(LossMSE, self).__init__()\n",
    "        pass\n",
    "        \n",
    "    def forward(self, y, target):\n",
    "        # out = e^2\n",
    "        # e = (y - f(x))\n",
    "        \n",
    "        self.y = y.clone()\n",
    "        target_onehot = torch.zeros((target.shape[0], 2)) \n",
    "        self.target = target_onehot.scatter_(1, target.view(-1, 1), 1)\n",
    "        \n",
    "        e = (self.y - self.target)\n",
    "        \n",
    "        \n",
    "        return e.pow(2).sum()\n",
    "        \n",
    "    def backward(self):\n",
    "        # out = 2 * e\n",
    "        \n",
    "        e = (self.y - self.target)\n",
    "        \n",
    "        return 2 * e\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def step(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, model, lr):\n",
    "        super(SGD, self).__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "    \n",
    "    # Train function?\n",
    "    # def train(..):\n",
    "    \n",
    "    def step(self):\n",
    "        model.update(self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    input = empty(nb, 2).uniform_(-1, 1)\n",
    "    target = input.pow(2).sum(1).sub(2 / math.pi).sign().add(1).div(2).long()\n",
    "    return input, target\n",
    "\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8121, -0.5705],\n",
       "        [-0.4460,  0.3026],\n",
       "        [ 0.1698,  0.1751],\n",
       "        ...,\n",
       "        [ 0.8959, -0.2452],\n",
       "        [-0.4758, -0.4332],\n",
       "        [-0.3906, -0.0852]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(train_input[0].view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, mini_batch_size=1):\n",
    "    criterion = LossMSE()\n",
    "    optimizer = SGD(model, lr = 1e-4)\n",
    "    nb_epochs = 50\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "        sum_loss = 0.\n",
    "        \n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            model.zero_grad()\n",
    "            output = model.forward(train_input.narrow(0, b, mini_batch_size))\n",
    "            #print(output)\n",
    "            loss = criterion.forward(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            \n",
    "            sum_loss += loss\n",
    "            \n",
    "            l_grad = criterion.backward()\n",
    "            model.backward(l_grad)\n",
    "            #print(model.param()[0])\n",
    "            optimizer.step()\n",
    "            \n",
    "        #print(model.param()[0])\n",
    "            \n",
    "        print(\"{} iteration: loss={}\".format(e, sum_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 iteration: loss=21529.341796875\n",
      "1 iteration: loss=1528.4969482421875\n",
      "2 iteration: loss=982.5928955078125\n",
      "3 iteration: loss=770.9544067382812\n",
      "4 iteration: loss=606.6292114257812\n",
      "5 iteration: loss=501.6761474609375\n",
      "6 iteration: loss=439.32672119140625\n",
      "7 iteration: loss=401.5660095214844\n",
      "8 iteration: loss=375.45794677734375\n",
      "9 iteration: loss=354.9000244140625\n",
      "10 iteration: loss=336.17498779296875\n",
      "11 iteration: loss=316.35107421875\n",
      "12 iteration: loss=299.86553955078125\n",
      "13 iteration: loss=286.371826171875\n",
      "14 iteration: loss=275.6252136230469\n",
      "15 iteration: loss=266.2290344238281\n",
      "16 iteration: loss=257.06878662109375\n",
      "17 iteration: loss=248.73333740234375\n",
      "18 iteration: loss=241.028076171875\n",
      "19 iteration: loss=234.10726928710938\n",
      "20 iteration: loss=228.12765502929688\n",
      "21 iteration: loss=221.9540557861328\n",
      "22 iteration: loss=215.96636962890625\n",
      "23 iteration: loss=210.80111694335938\n",
      "24 iteration: loss=206.25820922851562\n",
      "25 iteration: loss=202.05316162109375\n",
      "26 iteration: loss=198.12545776367188\n",
      "27 iteration: loss=194.48890686035156\n",
      "28 iteration: loss=191.18093872070312\n",
      "29 iteration: loss=188.1878204345703\n",
      "30 iteration: loss=185.19305419921875\n",
      "31 iteration: loss=182.48974609375\n",
      "32 iteration: loss=179.80747985839844\n",
      "33 iteration: loss=177.34397888183594\n",
      "34 iteration: loss=174.92874145507812\n",
      "35 iteration: loss=172.66024780273438\n",
      "36 iteration: loss=170.48428344726562\n",
      "37 iteration: loss=168.0418243408203\n",
      "38 iteration: loss=166.01898193359375\n",
      "39 iteration: loss=163.9492950439453\n",
      "40 iteration: loss=161.9236602783203\n",
      "41 iteration: loss=160.00601196289062\n",
      "42 iteration: loss=158.1524200439453\n",
      "43 iteration: loss=156.2179412841797\n",
      "44 iteration: loss=154.4440460205078\n",
      "45 iteration: loss=152.80856323242188\n",
      "46 iteration: loss=151.11376953125\n",
      "47 iteration: loss=149.62347412109375\n",
      "48 iteration: loss=148.318115234375\n",
      "49 iteration: loss=146.98329162597656\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(Linear(2, 25), ReLU(),\n",
    "                   Linear(25, 25), ReLU(),\n",
    "                   Linear(25, 25), ReLU(),\n",
    "                   Linear(25, 2))\n",
    "\n",
    "\n",
    "train_model(model, train_input, train_target, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_input, test_target):\n",
    "    num_samples = test_input.size(0)\n",
    "    prediction = model.forward(test_input)\n",
    "    predicted_class = torch.argmax(prediction, axis=1)\n",
    "    accuracy = torch.sum(predicted_class == test_target).float() / num_samples\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9340)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, test_input, test_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
