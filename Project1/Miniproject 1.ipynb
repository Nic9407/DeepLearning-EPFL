{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:22:40.796055Z",
     "start_time": "2020-04-01T13:22:40.792033Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from dlc_practical_prologue import generate_pair_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:30:50.709304Z",
     "start_time": "2020-04-01T14:30:50.400621Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 2])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input, train_target, train_classes, test_input, test_target, test_classes = generate_pair_sets(1000)\n",
    "train_classes.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:36:20.205516Z",
     "start_time": "2020-04-01T14:36:20.196681Z"
    }
   },
   "outputs": [],
   "source": [
    "class PairModel(nn.Module):\n",
    "    def __init__(self, nbch1=32, nbch2=64, nbfch=256, batch_norm=True):\n",
    "        super(PairModel, self).__init__()\n",
    "        self.nbch1 = nbch1\n",
    "        self.nbch2 = nbch2\n",
    "        self.nbfch = nbfch\n",
    "        self.batch_norm = batch_norm\n",
    "        self.conv1 = nn.Conv2d(2, nbch1, 3)\n",
    "        if batch_norm:\n",
    "            self.bn1 = nn.BatchNorm2d(nbch1)\n",
    "        self.conv2 = nn.Conv2d(nbch1, nbch2, 6)\n",
    "        if batch_norm:\n",
    "            self.bn2 = nn.BatchNorm2d(nbch2)\n",
    "        self.fc1 = nn.Linear(nbch2, nbfch)\n",
    "        self.fc2 = nn.Linear(nbfch, 2)\n",
    "    def forward(self, x):\n",
    "        if self.batch_norm:\n",
    "            x = F.relu(F.max_pool2d(self.bn1(self.conv1(x)), 2))\n",
    "            x = F.relu(self.bn2(self.conv2(x)))\n",
    "        else:\n",
    "            x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "            x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.fc1(x.view(-1, self.nbch2)))\n",
    "        return F.relu(self.fc2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:39:52.696713Z",
     "start_time": "2020-04-01T14:39:52.687735Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_pair_model(num_epochs=25, lr=0.1, mini_batch_size=100):\n",
    "    model = PairModel()\n",
    "    num_samples = train_input.size(0)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for e in range(num_epochs):\n",
    "        sum_loss = 0\n",
    "        for b in range(0, num_samples, mini_batch_size):\n",
    "            input_mini_batch = train_input[b:b + mini_batch_size]\n",
    "            target_mini_batch = train_target[b:b + mini_batch_size]\n",
    "            model.zero_grad()\n",
    "            prediction = model(input_mini_batch)\n",
    "            loss = criterion(prediction, target_mini_batch)\n",
    "            sum_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(e, sum_loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:42:55.858223Z",
     "start_time": "2020-04-01T14:42:55.845257Z"
    }
   },
   "outputs": [],
   "source": [
    "class SiameseModel(nn.Module):\n",
    "    def __init__(self, nbch1=32, nbch2=64, nbfch=256, batch_norm=True):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.nbch1 = nbch1\n",
    "        self.nbch2 = nbch2\n",
    "        self.nbfch = nbfch\n",
    "        self.batch_norm = batch_norm\n",
    "        self.conv1 = nn.Conv2d(1, nbch1, 3)\n",
    "        if batch_norm:\n",
    "            self.bn1 = nn.BatchNorm2d(nbch1)\n",
    "        self.conv2 = nn.Conv2d(nbch1, nbch2, 6)\n",
    "        if batch_norm:\n",
    "            self.bn2 = nn.BatchNorm2d(nbch2)\n",
    "        self.fc1 = nn.Linear(nbch2, nbfch)\n",
    "        self.fc2 = nn.Linear(nbfch, 10)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "    def forward(self, x):\n",
    "        if self.batch_norm:\n",
    "            x1, x2 = x[:, 0], x[:, 1]\n",
    "            x1 = x1.reshape(-1, 1, 14, 14)\n",
    "            x2 = x2.reshape(-1, 1, 14, 14)\n",
    "            x1 = F.relu(F.max_pool2d(self.bn1(self.conv1(x1)), 2))\n",
    "            x1 = F.relu(self.bn2(self.conv2(x1)))\n",
    "            x2 = F.relu(F.max_pool2d(self.bn1(self.conv1(x2)), 2))\n",
    "            x2 = F.relu(self.bn2(self.conv2(x2)))\n",
    "        else:\n",
    "            x1 = F.relu(F.max_pool2d(self.conv1(x1), 2))\n",
    "            x1 = F.relu(self.conv2(x1))\n",
    "            x2 = F.relu(F.max_pool2d(self.conv1(x2), 2))\n",
    "            x2 = F.relu(self.conv2(x2))\n",
    "        x1 = F.relu(self.fc1(x1.view(-1, self.nbch2)))\n",
    "        x1 = F.relu(self.fc2(x1))\n",
    "        x2 = F.relu(self.fc1(x2.view(-1, self.nbch2)))\n",
    "        x2 = F.relu(self.fc2(x2))\n",
    "        x = torch.cat((x1, x2), axis=1)\n",
    "        return F.relu(self.fc3(x)), (x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:41:51.060443Z",
     "start_time": "2020-04-01T14:41:51.051501Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_siamese_model(num_epochs=25, lr=0.1, mini_batch_size=100, loss_weights=(1, 1)):\n",
    "    model = SiameseModel()\n",
    "    num_samples = train_input.size(0)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for e in range(num_epochs):\n",
    "        sum_loss = 0\n",
    "        for b in range(0, num_samples, mini_batch_size):\n",
    "            input_mini_batch = train_input[b:b + mini_batch_size]\n",
    "            target_mini_batch = train_target[b:b + mini_batch_size]\n",
    "            classes_mini_batch = train_classes[b:b + mini_batch_size]\n",
    "            model.zero_grad()\n",
    "            prediction_2, (prediction_10_1, prediction_10_2) = model(input_mini_batch)\n",
    "            loss_2 = criterion(prediction_2, target_mini_batch)\n",
    "            loss_10_1 = criterion(prediction_10_1, classes_mini_batch[:, 0])\n",
    "            loss_10_2 = criterion(prediction_10_2, classes_mini_batch[:, 1])\n",
    "            loss_10 = loss_10_1 + loss_10_2\n",
    "            total_loss = loss_weights[0] * loss_2 + loss_weights[1] * loss_10\n",
    "            total_loss.backward()\n",
    "            sum_loss += total_loss.item()\n",
    "            optimizer.step()\n",
    "        print(e, sum_loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:35:00.011098Z",
     "start_time": "2020-04-01T14:35:00.007077Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "    num_samples = test_input.size(0)\n",
    "    prediction = model(test_input)\n",
    "    predicted_class = torch.argmax(prediction, axis=1)\n",
    "    accuracy = torch.sum(predicted_class == test_target).float() / num_samples\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:04:13.843749Z",
     "start_time": "2020-04-01T14:04:02.145191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.338943183422089\n",
      "1 5.194023281335831\n",
      "2 4.665180027484894\n",
      "3 4.361992746591568\n",
      "4 4.107274085283279\n",
      "5 3.9422988891601562\n",
      "6 3.3555320352315903\n",
      "7 2.6736945509910583\n",
      "8 1.6262310519814491\n",
      "9 0.9279737286269665\n",
      "10 1.3092701062560081\n",
      "11 1.5165461488068104\n",
      "12 0.33970838133245707\n",
      "13 0.06681734882295132\n",
      "14 0.029906735755503178\n",
      "15 0.020450577372685075\n",
      "16 0.0154412139672786\n",
      "17 0.012318151071667671\n",
      "18 0.010172725014854223\n",
      "19 0.008625649730674922\n",
      "20 0.00746550690382719\n",
      "21 0.006563328322954476\n",
      "22 0.005844814004376531\n",
      "23 0.005257547978544608\n",
      "24 0.004770920728333294\n"
     ]
    }
   ],
   "source": [
    "trained_pair_model = train_pair_model(lr=1e0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:04:40.115697Z",
     "start_time": "2020-04-01T14:04:40.036907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8260)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(trained_pair_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:55:17.470674Z",
     "start_time": "2020-04-01T14:54:50.223405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 18.91983151435852\n",
      "1 12.93790578842163\n",
      "2 8.380740702152252\n",
      "3 6.232254892587662\n",
      "4 5.683494448661804\n",
      "5 4.304722011089325\n",
      "6 3.3988476544618607\n",
      "7 3.291791617870331\n",
      "8 2.376682296395302\n",
      "9 1.5707224905490875\n",
      "10 2.0111621767282486\n",
      "11 2.6959504559636116\n",
      "12 3.1543694511055946\n",
      "13 2.5462238863110542\n",
      "14 0.8878308944404125\n",
      "15 0.593383114784956\n",
      "16 0.32002727687358856\n",
      "17 0.2417464954778552\n",
      "18 0.20591144356876612\n",
      "19 0.18339358922094107\n",
      "20 0.1670431005768478\n",
      "21 0.15462171332910657\n",
      "22 0.14506843546405435\n",
      "23 0.1377294142730534\n",
      "24 0.132113853469491\n"
     ]
    }
   ],
   "source": [
    "trained_siamese_model = train_siamese_model(lr=1, loss_weights=(1.5, 0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:47:25.589351Z",
     "start_time": "2020-04-01T14:47:25.583332Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_siamese_model(model):\n",
    "    num_samples = test_input.size(0)\n",
    "    prediction_2, (prediction_10_1, prediction_10_2) = model(test_input)\n",
    "    predicted_class_2 = torch.argmax(prediction_2, axis=1)\n",
    "    predicted_class_10_1 = torch.argmax(prediction_10_1, axis=1)\n",
    "    predicted_class_10_2 = torch.argmax(prediction_10_2, axis=1)\n",
    "    predicted_class_10 = predicted_class_10_1 <= predicted_class_10_2\n",
    "    accuracy_2 = torch.sum(predicted_class_2 == test_target).float() / num_samples\n",
    "    accuracy_10 = torch.sum(predicted_class_10 == test_target).float() / num_samples\n",
    "    return accuracy_2, accuracy_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:55:36.352588Z",
     "start_time": "2020-04-01T14:55:36.213960Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9280), tensor(0.9690))"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_siamese_model(trained_siamese_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### ToDo\n",
    " - gradient norm over depth vs usage of batch_norm\n",
    " - cross-val of learn_rate, siamese_loss_weights, num_channels, num_linear_hidden\n",
    " - one loss test accuracy vs two losses(augmented) test accuracy\n",
    " - possibly, 20->2 probs with linear vs non-linear transform\n",
    " - impact of initializations, normalizing input, adding skip connections, dropout...?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
